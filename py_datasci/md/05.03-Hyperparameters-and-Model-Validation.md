
# Hyperparameters and Model Validation

Basic steps for applying a supervised machine learning model:

1. Choose model
2. Choose model hyperparameters
3. Fit the model to the training data
4. Use the model to predict labels for new data


```python
from sklearn.datasets import load_iris
iris = load_iris()
X = iris.data[:,:2]
y = iris.target
```


```python
from matplotlib.colors import ListedColormap
h = .02  # step size in the mesh
n=15

cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])

cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])

for weights in ['uniform', 'distance']:
    clf = KNeighborsClassifier(n_neighbors=n, weights=weights)
    clf.fit(X, y)

    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                         np.arange(y_min, y_max, h))
    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])

    Z = Z.reshape(xx.shape)
    plt.figure(figsize=(15,8))
    plt.pcolormesh(xx, yy, Z, cmap=cmap_light)

    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold)
    plt.xlim(xx.min(), xx.max())
    plt.ylim(yy.min(), yy.max())
    plt.title("3-Class classification (k = %i, weights = '%s')"
              % (n, weights))

plt.show()
```


![png](https://raw.githubusercontent.com/karenyyy/data_science/master/py_datasci/images/output_3_0.png)



![png](https://raw.githubusercontent.com/karenyyy/data_science/master/py_datasci/images/output_3_1.png)



```python
from sklearn.cross_validation import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score

X1, X2, y1, y2 = train_test_split(X, y, random_state=0,
                                  train_size=0.7)
model=KNeighborsClassifier(n_neighbors=10)
model.fit(X1, y1)

y2_model = model.predict(X2)
accuracy_score(y2, y2_model)
```




   0.73333333333333328




```python
from sklearn.cross_validation import cross_val_score
cross_val_score(model, X, y, cv=10).mean()
```




   0.76666666666666672



Repeating the validation across different subsets of the data gives us an even better idea of the performance of the algorithm.

Scikit-Learn implements a number of useful cross-validation schemes that are implemented via iterators in the ``cross_validation`` module.

For example, __LeaveOneOut:(really extrme case: all used for training leaving only one for testing)__


```python
from sklearn.cross_validation import LeaveOneOut
scores = cross_val_score(model, X, y, cv=LeaveOneOut(len(X)))
scores
```




    array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
            1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
            1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
            1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
            1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
            1.,  1.,  1.,  1.,  1.,  0.,  1.,  0.,  1.,  1.,  1.,  1.,  1.,
            1.,  1.,  1.,  1.,  1.,  0.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
            1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
            1.,  1.,  0.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
            1.,  1.,  0.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
            1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
            1.,  1.,  1.,  1.,  1.,  1.,  1.])




```python
scores.mean()
```




    0.95999999999999996



take a look at Scikit-Learn's online [cross-validation documentation](http://scikit-learn.org/stable/modules/cross_validation.html).

## Selecting the Best Model
> if our estimator is underperforming, how should we move forward?


There are several possible answers:

- Use a more complicated/more flexible model
- Use a less complicated/less flexible model
- Gather more training samples
- Gather more data to add features to each sample

In particular, sometimes using a more complicated model will give worse results, and adding more training samples may not improve your results.

### The Bias-variance trade-off

Fundamentally, the question of "the best model" is about finding a balanced spot in the tradeoff between *bias* and *variance*.



- For high-bias models, the performance of the model on the validation set is similar to the performance on the training set.
- For high-variance models, the performance of the model on the validation set is far worse than the performance on the training set.


```python
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.pipeline import make_pipeline

def PolynomialRegression(degree=2, **kwargs):
    return make_pipeline(PolynomialFeatures(degree),
                         LinearRegression(**kwargs))
```


```python
import numpy as np

def make_data(N, err=1.0, seed=1):
    X = np.random.RandomState(seed).rand(N, 1) ** 2
    y = 10 - 1. / (X.ravel() + 0.1) 
    if err > 0:
        y += err * np.random.RandomState(seed).randn(N) # add noise
    return X, y

X, y = make_data(40)
```


```python
%matplotlib inline
import matplotlib.pyplot as plt
import seaborn; seaborn.set()  
fig=plt.figure(figsize=(15,8))

X_test = np.linspace(-0.1, 1.1, 500)[:, None]

fig = plt.figure(figsize=(15, 8))

plt.scatter(X.ravel(), y, color='black')

axis = plt.axis()

for degree in [1, 3, 5, 7, 9, 11]:
    y_test = PolynomialRegression(degree).fit(X, y).predict(X_test)
    plt.plot(X_test.ravel(), y_test, label='degree={0}'.format(degree))
    
    
plt.xlim(-0.1, 1.0)
plt.ylim(-2, 12)
plt.legend(loc='lower right');
```


    <matplotlib.figure.Figure at 0x7f01c49d24a8>



![png](https://raw.githubusercontent.com/karenyyy/data_science/master/py_datasci/images/output_15_1.png)


>  what degree of polynomial provides a suitable trade-off between bias (under-fitting) and variance (over-fitting)?




```python
xfit = np.linspace(-0.1, 1.0, 1000)[:, None]
model1 = PolynomialRegression(1).fit(X, y)
model20 = PolynomialRegression(20).fit(X, y)

fig, ax = plt.subplots(1, 2, figsize=(16, 6))
fig.subplots_adjust(left=0.0625, right=0.95, wspace=0.1)

ax[0].scatter(X.ravel(), y, s=40)
ax[0].plot(xfit.ravel(), model1.predict(xfit), color='red')
ax[0].axis([-0.1, 1.0, -2, 14])
ax[0].set_title('High-bias model: Underfits the data', size=14)

ax[1].scatter(X.ravel(), y, s=40)
ax[1].plot(xfit.ravel(), model20.predict(xfit), color='red')
ax[1].axis([-0.1, 1.0, -2, 14])
ax[1].set_title('High-variance model: Overfits the data', size=14)
```




    Text(0.5,1,'High-variance model: Overfits the data')




![png](https://raw.githubusercontent.com/karenyyy/data_science/master/py_datasci/images/output_17_1.png)


We can make further progress in this by visualizing the validation curve for this particular data and model

this can be done straightforwardly using the ``validation_curve`` convenience routine provided by Scikit-Learn.


Given a model, data, parameter name, and a range to explore, this function will automatically compute both the training score and validation score across the range:


```python
x = np.linspace(0, 1, 1000)
y1 = -(x - 0.5) ** 2
y2 = y1 - 0.33 + np.exp(x - 1)

fig, ax = plt.subplots(figsize=(16,8))
ax.plot(x, y2, lw=10, alpha=0.5, color='blue')
ax.plot(x, y1, lw=10, alpha=0.5, color='red')

ax.text(0.15, 0.2, "training score", rotation=45, size=16, color='blue')
ax.text(0.2, -0.05, "validation score", rotation=20, size=16, color='red')

ax.text(0.02, 0.1, r'$\longleftarrow$ High Bias', size=18, rotation=90, va='center')
ax.text(0.98, 0.1, r'$\longleftarrow$ High Variance $\longrightarrow$', size=18, rotation=90, ha='right', va='center')
ax.text(0.48, -0.12, 'Best$\\longrightarrow$\nModel', size=18, rotation=90, va='center')

ax.set_xlim(0, 1)
ax.set_ylim(-0.3, 0.5)

ax.set_xlabel(r'model complexity $\longrightarrow$', size=14)
ax.set_ylabel(r'model score $\longrightarrow$', size=14)

ax.xaxis.set_major_formatter(plt.NullFormatter())
ax.yaxis.set_major_formatter(plt.NullFormatter())

ax.set_title("Validation Curve Schematic", size=16)
```




    Text(0.5,1,'Validation Curve Schematic')




![png](https://raw.githubusercontent.com/karenyyy/data_science/master/py_datasci/images/output_19_1.png)



```python
N = np.linspace(0, 1, 1000)
y1 = 0.75 + 0.2 * np.exp(-4 * N)
y2 = 0.7 - 0.6 * np.exp(-4 * N)

fig, ax = plt.subplots(figsize=(16,8))
ax.plot(x, y1, lw=10, alpha=0.5, color='blue')
ax.plot(x, y2, lw=10, alpha=0.5, color='red')

ax.text(0.2, 0.88, "training score", rotation=-10, size=16, color='blue')
ax.text(0.2, 0.5, "validation score", rotation=30, size=16, color='red')

ax.text(0.98, 0.45, r'Good Fit $\longrightarrow$', size=18, rotation=90, ha='right', va='center')
ax.text(0.02, 0.57, r'$\longleftarrow$ High Variance $\longrightarrow$', size=18, rotation=90, va='center')

ax.set_xlim(0, 1)
ax.set_ylim(0, 1)

ax.set_xlabel(r'training set size $\longrightarrow$', size=14)
ax.set_ylabel(r'model score $\longrightarrow$', size=14)

ax.xaxis.set_major_formatter(plt.NullFormatter())
ax.yaxis.set_major_formatter(plt.NullFormatter())

ax.set_title("Learning Curve Schematic", size=16)
```




    Text(0.5,1,'Learning Curve Schematic')




![png](https://raw.githubusercontent.com/karenyyy/data_science/master/py_datasci/images/output_20_1.png)



```python
from sklearn.learning_curve import validation_curve
fig=plt.figure(figsize=(16,8))
degree = np.arange(0, 20)
train_score, val_score = validation_curve(PolynomialRegression(), X, y,
                                          'polynomialfeatures__degree', degree, cv=10)

plt.plot(degree, np.median(train_score, 1), color='blue', label='training score', linewidth=5)
plt.plot(degree, np.median(val_score, 1), color='red', label='validation score', linewidth=5)
plt.legend(loc='best')
plt.ylim(0, 1)
plt.xlabel('degree')
plt.ylabel('score');
```


![png](https://raw.githubusercontent.com/karenyyy/data_science/master/py_datasci/images/output_21_0.png)


This shows the training score is everywhere higher than the validation score; 

- the training score is monotonically improving with increased model complexity; 
- and the validation score reaches a maximum before dropping off as the model becomes over-fit.

From the validation curve, we can read-off that the optimal trade-off between bias and variance is found for probably 4th ~ 6th-order polynomial; we can compute and display this fit over the original data as follows:


```python
fig=plt.figure(figsize=(16,8))
plt.scatter(X.ravel(), y)
lim = plt.axis()
for i in range(4,7):
    y_test = PolynomialRegression(i).fit(X, y).predict(X_test)
    plt.plot(X_test.ravel(), y_test, label="degree{}".format(i));
plt.axis(lim);
plt.legend(loc='lower right')
```




    <matplotlib.legend.Legend at 0x7f01c2ad75c0>




![png](https://raw.githubusercontent.com/karenyyy/data_science/master/py_datasci/images/output_23_1.png)



```python
fig, ax = plt.subplots(1, 2, figsize=(17, 6))
fig.subplots_adjust(left=0.0625, right=0.95, wspace=0.1)

X2, y2 = make_data(10, seed=2018)

ax[0].scatter(X.ravel(), y, s=40, c='blue')
ax[0].plot(xfit.ravel(), model1.predict(xfit), color='gray', linestyle='--')
ax[0].axis([-0.1, 1.0, -2, 14])
ax[0].set_title('High-bias model: Underfits the data', size=14)
ax[0].scatter(X2.ravel(), y2, s=40, c='red')
ax[0].text(0.02, 0.98, "training score: $R^2$ = {0:.2f}".format(model1.score(X, y)),
           ha='left', va='top', transform=ax[0].transAxes, size=14, color='blue')
ax[0].text(0.02, 0.91, "validation score: $R^2$ = {0:.2f}".format(model1.score(X2, y2)),
           ha='left', va='top', transform=ax[0].transAxes, size=14, color='red')

ax[1].scatter(X.ravel(), y, s=40, c='blue')
ax[1].plot(xfit.ravel(), model20.predict(xfit), color='gray', linestyle='--')
ax[1].axis([-0.1, 1.0, -2, 14])
ax[1].set_title('High-variance model: Overfits the data', size=14)
ax[1].scatter(X2.ravel(), y2, s=40, c='red')
ax[1].text(0.02, 0.98, "training score: $R^2$ = {0:.2g}".format(model20.score(X, y)),
           ha='left', va='top', transform=ax[1].transAxes, size=14, color='blue')
ax[1].text(0.02, 0.91, "validation score: $R^2$ = {0:.2g}".format(model20.score(X2, y2)),
           ha='left', va='top', transform=ax[1].transAxes, size=14, color='red') 
# The use of transform=ax.transAxes throughout the code indicates that the coordinates are given relative 
# to the axes bounding box, with 0,0 being the lower left of the axes and 1,1 the upper right.
```




    Text(0.02,0.91,'validation score: $R^2$ = -2.3')




![png](https://raw.githubusercontent.com/karenyyy/data_science/master/py_datasci/images/output_24_1.png)


## Learning Curves

One important aspect of model complexity is that the optimal model will generally depend on the size of your training data.


```python
X2, y2 = make_data(200)
fig=plt.figure(figsize=(15,8))
plt.scatter(X2.ravel(), y2);
```


![png](https://raw.githubusercontent.com/karenyyy/data_science/master/py_datasci/images/output_26_0.png)



```python
fig=plt.figure(figsize=(15,8))
degree = np.arange(20)
train_score2, val_score2 = validation_curve(PolynomialRegression(), X2, y2,
                                            'polynomialfeatures__degree', degree, cv=10)

plt.plot(degree, np.median(train_score2, 1), color='blue', label='training score')
plt.plot(degree, np.median(val_score2, 1), color='red', label='validation score')
plt.plot(degree, np.median(train_score, 1), color='blue', alpha=0.3, linestyle='dashed')
plt.plot(degree, np.median(val_score, 1), color='red', alpha=0.3, linestyle='dashed')
plt.legend(loc='lower center')
plt.ylim(0, 1)
plt.xlabel('degree')
plt.ylabel('score');
```


![png](https://raw.githubusercontent.com/karenyyy/data_science/master/py_datasci/images/output_27_0.png)


It is clear from the validation curve that the larger dataset can support a much more complicated model: 

- the peak here is probably around a degree of 6
- but even a degree-20 model is not seriously over-fitting the data
    - the validation and training scores remain very close.

Thus we see that the behavior of the validation curve has two important inputs: the model complexity and the number of training points.

It is often useful to to explore the behavior of the model as a function of the number of training points, which we can do by using increasingly larger subsets of the data to fit our model.

The general behavior we would expect from a learning curve is this:

- A model of a given complexity will *overfit* a small dataset: this means the training score will be relatively high, while the validation score will be relatively low.
- A model of a given complexity will *underfit* a large dataset: this means that the training score will decrease, but the validation score will increase.
- __(Attention!!!)__ A model will never, except by chance, give a better score to the validation set than the training set: this means the curves should keep getting closer together but never cross.



__In particular, once you have enough points that a particular model has converged, *adding more training data will not help!*__

The only way to increase model performance in this case is to use another _(often more complex)_ model.

### Learning curves in Scikit-Learn

Scikit-Learn offers a convenient utility for computing such learning curves from your models; here we will compute a learning curve for our original dataset with a second-order polynomial model and a ninth-order polynomial:


```python
from sklearn.learning_curve import learning_curve

fig, ax = plt.subplots(1, 4, figsize=(15, 5))
fig.subplots_adjust(left=0.0625, right=0.95, wspace=0.1)

for i, degree in enumerate([2,4,6,8]):
    N, train_lc, val_lc = learning_curve(PolynomialRegression(degree),
                                         X, y, 
                                         cv=10,
                                         train_sizes=np.linspace(0.3, 1, 25))

    ax[i].plot(N, np.mean(train_lc, 1), color='blue', label='training score')
    ax[i].plot(N, np.mean(val_lc, 1), color='red', label='validation score')
    ax[i].hlines(np.mean([train_lc[-1], val_lc[-1]]), N[0], N[-1],
                 color='gray', linestyle='dashed')

    ax[i].set_ylim(-50, 1)
    ax[i].set_xlim(N[0], N[-1])
    ax[i].set_xlabel('training size')
    ax[i].set_ylabel('score')
    ax[i].set_title('degree = {0}'.format(degree), size=14)
    ax[i].legend(loc='best')
```


![png](https://raw.githubusercontent.com/karenyyy/data_science/master/py_datasci/images/output_31_0.png)


The diagnostics above gives us a visual depiction of how our model responds to increasing training data.


In particular, when your learning curve has already converged (i.e., when the training and validation curves are already close to each other) *adding more training data will not significantly improve the fit*

This situation is seen in the first panel, with the learning curve for the degree-2 model.

By moving to a much more complicated model, we increase the score of convergence (indicated by the dashed line), but at the expense of higher model variance _(indicated by the difference between the training and validation scores)_.

If we were to add even more data points, the learning curve for the more complicated model would eventually converge.

__Plotting a learning curve for your particular choice of model and dataset can help you to make this type of decision about how to move forward in improving your analysis.__

## Validation in Practice: Grid Search

In practice, models generally have more than one knob to turn, and thus plots of validation and learning curves change from lines to multi-dimensional surfaces.

> Solutions?

In these cases, such visualizations are difficult and we would rather simply find the particular model that maximizes the validation score.

Scikit-Learn provides automated tools to do this in the grid search module.


- Using grid search:
    - This can be set up using Scikit-Learn's ``GridSearchCV`` meta-estimator:


```python
from sklearn.grid_search import GridSearchCV

param_grid = {'polynomialfeatures__degree': np.arange(20),
              'linearregression__fit_intercept': [True, False],
              'linearregression__normalize': [True, False]}

grid = GridSearchCV(PolynomialRegression(), param_grid, cv=10)
```


```python
grid.fit(X, y)
```

Now that this is fit, we can ask for the best parameters as follows:


```python
grid.best_params_
```




    {'linearregression__fit_intercept': True,
     'linearregression__normalize': False,
     'polynomialfeatures__degree': 6}





# Feature Engineering

__vectorization__: converting arbitrary data into well-behaved vectors

In the real world, data rarely comes in a tidy, ``[n_samples, n_features]`` format.

With this in mind, one of the more important steps in using machine learning in practice is *feature engineering*: that is, taking whatever information you have about your problem and turning it into numbers that you can use to build your feature matrix.

- features for representing *categorical data*
- features for representing *text*
- features for representing *images*

-  *derived features* for increasing model complexity 
- *imputation* of missing data.


## Categorical Features

One common type of non-numerical data is *categorical* data.
For example, imagine you are exploring some data on housing prices, and along with numerical features like "price" and "rooms", you also have "neighborhood" information.
For example, your data might look something like this:


```python
data = [
    {'price': 850000, 'rooms': 4, 'neighborhood': 'Queen Anne'},
    {'price': 700000, 'rooms': 3, 'neighborhood': 'Fremont'},
    {'price': 650000, 'rooms': 3, 'neighborhood': 'Wallingford'},
    {'price': 600000, 'rooms': 2, 'neighborhood': 'Fremont'}
]
```


```python
{'Queen Anne': 1, 'Fremont': 2, 'Wallingford': 3};
```

To avoid making the fundamental assumption that numerical features reflect algebraic quantities, use *one-hot encoding*.

Scikit-Learn's ``DictVectorizer`` will do this:


```python
from sklearn.feature_extraction import DictVectorizer
vec = DictVectorizer(sparse=False, dtype=int)
vec.fit_transform(data)
```




    array([[     0,      1,      0, 850000,      4],
           [     1,      0,      0, 700000,      3],
           [     0,      0,      1, 650000,      3],
           [     1,      0,      0, 600000,      2]], dtype=int64)



Notice that the 'neighborhood' column has been expanded into three separate columns, representing the three neighborhood labels, and that each row has a 1 in the column associated with its neighborhood.


```python
vec.get_feature_names()
```




    ['neighborhood=Fremont',
     'neighborhood=Queen Anne',
     'neighborhood=Wallingford',
     'price',
     'rooms']



There is one clear disadvantage of this approach: __if your category has many possible values, this can *greatly* increase the size of your dataset.__

However, because the encoded data __contains mostly zeros__, a __sparse https://raw.githubusercontent.com/karenyyy/data_science/master/py_datasci/images/output__ can be a very efficient solution:


```python
vec = DictVectorizer(sparse=True, dtype=int)
out=vec.fit_transform(data)
out
```




    <4x5 sparse matrix of type '<class 'numpy.int64'>'
    	with 12 stored elements in Compressed Sparse Row format>




```python
print(out)
```

      (0, 1)	1
      (0, 3)	850000
      (0, 4)	4
      (1, 0)	1
      (1, 3)	700000
      (1, 4)	3
      (2, 2)	1
      (2, 3)	650000
      (2, 4)	3
      (3, 0)	1
      (3, 3)	600000
      (3, 4)	2


Other sklearn encoders:

- ``sklearn.preprocessing.OneHotEncoder`` 
- ``sklearn.feature_extraction.FeatureHasher`` 

## Text Features

Another common need in feature engineering is to __convert text to a set of representative numerical values__.

For example, most automatic mining of social media data relies on some form of encoding the text as numbers.

One of the simplest methods of encoding data is by *word counts*:


```python
sample = ['problem of evil',
          'evil queen',
          'horizon problem']
```

For a vectorization of this data based on word count, we could construct a column representing the word "problem," the word "evil," the word "horizon," and so on.

using Scikit-Learn's ``CountVectorizer``:


```python
from sklearn.feature_extraction.text import CountVectorizer

vec = CountVectorizer()
X = vec.fit_transform(sample)
print(X)
```

      (0, 3)	1
      (0, 2)	1
      (0, 0)	1
      (1, 0)	1
      (1, 4)	1
      (2, 3)	1
      (2, 1)	1


The result is a sparse matrix recording the number of times each word appears; it is easier to inspect if we convert this to a ``DataFrame`` with labeled columns:


```python
import pandas as pd
# index: which sentence
# each column: whether the key of this column appear in all the sentences (indices), yes:1, no:0
pd.DataFrame(X.toarray(), columns=vec.get_feature_names())
```




<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>evil</th>
      <th>horizon</th>
      <th>of</th>
      <th>problem</th>
      <th>queen</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>



There are some issues with this approach, however: the raw word counts lead to features which __put too much weight on words that appear very frequently__, and this can be sub-optimal in some classification algorithms.


One approach to fix this is known as __*term frequency-inverse document frequency* (*TFâ€“IDF*) which weights the word counts by a measure of how often they appear in the documents:__




```python
from sklearn.feature_extraction.text import TfidfVectorizer
vec = TfidfVectorizer()
X = vec.fit_transform(sample)
pd.DataFrame(X.toarray(), columns=vec.get_feature_names())
```




<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>evil</th>
      <th>horizon</th>
      <th>of</th>
      <th>problem</th>
      <th>queen</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.517856</td>
      <td>0.000000</td>
      <td>0.680919</td>
      <td>0.517856</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.605349</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.795961</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.000000</td>
      <td>0.795961</td>
      <td>0.000000</td>
      <td>0.605349</td>
      <td>0.000000</td>
    </tr>
  </tbody>
</table>
</div>



## Image Features

Another common need for vectorization is to suitably encode *images* for machine learning analysis.

find excellent implementations of many of the standard approaches in the [Scikit-Image project](http://scikit-image.org).

## Derived Features

Another useful type of feature is one that is mathematically derived from some input features.


For example, this data clearly cannot be well described by a straight line:


```python
%matplotlib inline
import numpy as np
import matplotlib.pyplot as plt

x = np.array([1, 2, 3, 4, 5])
y = np.array([4, 2, 1, 3, 7])
plt.scatter(x, y);
```


![png](https://raw.githubusercontent.com/karenyyy/data_science/master/py_datasci/images/output_23_0.png)


Still, we can fit a line to the data using ``LinearRegression`` and get the optimal result:


```python
x
```




    array([1, 2, 3, 4, 5])




```python
x[:, np.newaxis]
```




    array([[1],
           [2],
           [3],
           [4],
           [5]])




```python
from sklearn.linear_model import LinearRegression
X = x[:, np.newaxis]
model = LinearRegression().fit(X, y)
yfit = model.predict(X)
plt.scatter(x, y)
plt.plot(x, yfit);
```


![png](https://raw.githubusercontent.com/karenyyy/data_science/master/py_datasci/images/lr.png)


It's clear that we need a more sophisticated model to describe the relationship between $x$ and $y$.

One approach to this is to transform the data, adding extra columns of features to drive more flexibility in the model.
For example, we can add polynomial features to the data:


```python
from sklearn.preprocessing import PolynomialFeatures
poly = PolynomialFeatures(degree=3, include_bias=True)
X2 = poly.fit_transform(X)
print(X2)
```

    [[   1.    1.    1.    1.]
     [   1.    2.    4.    8.]
     [   1.    3.    9.   27.]
     [   1.    4.   16.   64.]
     [   1.    5.   25.  125.]]


The derived feature matrix has one column representing $x$, and a second column representing $x^2$, and a third column representing $x^3$.
Computing a linear regression on this expanded input gives a much closer fit to our data:


```python
model = LinearRegression().fit(X2, y)
yfit = model.predict(X2)
plt.scatter(x, y)
plt.plot(x, yfit);
```


![png](https://raw.githubusercontent.com/karenyyy/data_science/master/py_datasci/images/lr2.png)


__This idea of improving a model not by changing the model, but by transforming the inputs, is fundamental to many of the more powerful machine learning methods.__

More generally, this is one motivational path to the powerful set of techniques known as **kernel methods**

## Imputation of Missing Data

Another common need in feature engineering is handling of missing data.


```python
from numpy import nan
X = np.array([[ nan, 0,   3  ],
              [ 3,   7,   9  ],
              [ 3,   5,   2  ],
              [ 4,   nan, 6  ],
              [ 8,   8,   1  ]])
y = np.array([14, 16, -1,  8, -5])
```

When applying a typical machine learning model to such data, we will need to first replace such missing data with some appropriate fill value.
This is known as **imputation** of missing values, and _strategies range from simple (e.g., replacing missing values with the mean of the column) to sophisticated (e.g., using matrix completion or a robust model to handle such data)._

__For a baseline imputation approach, using the mean, median, or most frequent value__, Scikit-Learn provides the ``Imputer`` class:


```python
from sklearn.preprocessing import Imputer
imp = Imputer(strategy='median')
X2 = imp.fit_transform(X)
X2
```




    array([[ 3.5,  0. ,  3. ],
           [ 3. ,  7. ,  9. ],
           [ 3. ,  5. ,  2. ],
           [ 4. ,  6. ,  6. ],
           [ 8. ,  8. ,  1. ]])



We see that in the resulting data, the two missing values have been replaced with the median of the remaining values in the column. This imputed data can then be fed directly into, for example, a ``LinearRegression`` estimator:


```python
model = LinearRegression().fit(X2, y)
model.predict(X2)
```




    array([ 13.77424796,  15.08489797,  -1.1491576 ,   9.58429559,  -5.29428391])



## Feature Pipelines
Eg:

1. Impute missing values using the mean
2. Transform features to quadratic
3. Fit a linear regression

To streamline this type of processing pipeline, Scikit-Learn provides a ``Pipeline`` object, which can be used as follows:


```python
from sklearn.pipeline import make_pipeline

model = make_pipeline(Imputer(strategy='mean'),
                      PolynomialFeatures(degree=2),
                      LinearRegression())
```


```python
model.fit(X, y)  
print(y)
print(model.predict(X))
```

    [14 16 -1  8 -5]
    [ 14.  16.  -1.   8.  -5.]

<style>
   h1,
    h2,
    h3,
    h4,
    h5,
    h6,
    p,
    blockquote {
        margin: 0;
        padding: 0;
    }
    body {
        font-family: "Helvetica Neue", Helvetica, "Hiragino Sans GB", Arial, sans-serif;
        font-size: 13px;
        line-height: 18px;
        color: #737373;
        background-color: white;
        margin: 10px 13px 10px 13px;
    }
    table {
        margin: 10px 0 15px 0;
        border-collapse: collapse;
    }
    td,th {
        border: 1px solid #ddd;
        padding: 3px 10px;
    }
    th {
        padding: 5px 10px;
    }

    a {
        color: #0069d6;
    }
    a:hover {
        color: #0050a3;
        text-decoration: none;
    }
    a img {
        border: none;
    }
    p {
        margin-bottom: 9px;
    }
    h1,
    h2,
    h3,
    h4,
    h5,
    h6 {
        color: #404040;
        line-height: 36px;
    }
    h1 {
        margin-bottom: 18px;
        font-size: 30px;
    }
    h2 {
        font-size: 24px;
    }
    h3 {
        font-size: 18px;
    }
    h4 {
        font-size: 16px;
    }
    h5 {
        font-size: 14px;
    }
    h6 {
        font-size: 13px;
    }
    hr {
        margin: 0 0 19px;
        border: 0;
        border-bottom: 1px solid #ccc;
    }
    blockquote {
        padding: 13px 13px 21px 15px;
        margin-bottom: 18px;
        font-family:georgia,serif;
        font-style: italic;
    }
    blockquote:before {
        content:"\201C";
        font-size:40px;
        margin-left:-10px;
        font-family:georgia,serif;
        color:#eee;
    }
    blockquote p {
        font-size: 14px;
        font-weight: 300;
        line-height: 18px;
        margin-bottom: 0;
        font-style: italic;
    }
    code, pre {
        font-family: Monaco, Andale Mono, Courier New, monospace;
    }
    code {
        background-color: #fee9cc;
        color: rgba(0, 0, 0, 0.75);
        padding: 1px 3px;
        font-size: 12px;
        -webkit-border-radius: 3px;
        -moz-border-radius: 3px;
        border-radius: 3px;
    }
    pre {
        display: block;
        padding: 14px;
        margin: 0 0 18px;
        line-height: 16px;
        font-size: 11px;
        border: 1px solid #d9d9d9;
        white-space: pre-wrap;
        word-wrap: break-word;
    }
    pre code {
        background-color: #fff;
        color:#737373;
        font-size: 11px;
        padding: 0;
    }
    sup {
        font-size: 0.83em;
        vertical-align: super;
        line-height: 0;
    }
    * {
        -webkit-print-color-adjust: exact;
    }
    @media screen and (min-width: 914px) {
        body {
            width: 854px;
            margin:10px auto;
        }
    }
    @media print {
        body,code,pre code,h1,h2,h3,h4,h5,h6 {
            color: black;
        }
        table, pre {
            page-break-inside: avoid;
        }
    }
    
</style>