

```python
%matplotlib inline
import matplotlib.pyplot as plt
plt.rcParams["figure.figsize"] = (16,8)
import seaborn as sns; sns.set()
import numpy as np
```


```python
def toy_sample(N, rseed=1):
    # Make a plot with "MANIFOLD" text
    fig, ax = plt.subplots()
    fig.subplots_adjust(left=0, right=1, bottom=0, top=1)
    ax.axis('off')
    ax.text(0.5, 0.4, 'MANIFOLD', va='center', ha='center', weight='bold', size=85)
    fig.savefig('MANIFOLD.png')
    plt.close(fig)
    
    # Open this PNG and draw random points from it
    from matplotlib.image import imread
    data = imread('MANIFOLD.png')[::-1, :, 0].T
    rng = np.random.RandomState(rseed)
    X = rng.rand(4 * N, 2)
    i, j = (X * data.shape).astype(int).T
    mask = (data[i, j] < 1)
    X = X[mask]
    X[:, 0] *= (data.shape[0] / data.shape[1])
    X = X[:N]
    return X[np.argsort(X[:, 0])]
```


```python
X = toy_sample(20000)
colorize = dict(c=X[:, 0], cmap=plt.cm.get_cmap('rainbow', 8))
plt.scatter(X[:, 0], X[:, 1], **colorize)
plt.axis('equal');
```


![png](output_2_0.png)


## Multidimensional Scaling (MDS)

Looking at data like this, we can see that the particular choice of *x* and *y* values of the dataset are not the most fundamental description of the data: we can scale, shrink, or rotate the data, and the "HELLO" will still be apparent.
For example, if we use a rotation matrix to rotate the data, the *x* and *y* values change, but the data is still fundamentally the same:


```python
def rotate(X, angle):
    theta = np.deg2rad(angle)
    R = [[np.cos(theta), np.sin(theta)],
         [-np.sin(theta), np.cos(theta)]]
    return np.dot(X, R)
    
X2 = rotate(X, 20) + 5
plt.scatter(X2[:, 0], X2[:, 1], **colorize)
plt.axis('equal');
```


![png](output_4_0.png)


__This tells us that the *x* and *y* values are not necessarily fundamental to the relationships in the data.
What *is* fundamental, in this case, is the *distance* between each point and the other points in the dataset.__

A common way to represent this is to use a __distance matrix__: for N points, we construct an `N x N` array such that entry (i, j) contains the distance between point i and point j.


```python
from sklearn.metrics import pairwise_distances
D = pairwise_distances(X)
D.shape
```




    (2035, 2035)




```python
plt.imshow(D, zorder=2, cmap='Blues', interpolation='nearest')
plt.colorbar();
```


![png](output_7_0.png)


If we similarly construct a distance matrix for our rotated and translated data, we see that it is the same:


```python
D2 = pairwise_distances(X2)
np.allclose(D, D2)
```




    True



__This distance matrix gives us a representation of our data that is invariant to rotations and translations__, but the visualization of the matrix above is not entirely intuitive.



__Further, while computing this distance matrix from the (x, y) coordinates is straightforward, transforming the distances back into *x* and *y* coordinates is rather difficult.__


This is exactly what the multidimensional scaling algorithm aims to do: __given a distance matrix between points, it recovers a D-dimensional coordinate representation of the data.__




```python
from sklearn.manifold import MDS
model = MDS(n_components=2, dissimilarity='precomputed', random_state=1)
out = model.fit_transform(D)
plt.scatter(out[:, 0], out[:, 1], **colorize)
plt.axis('equal');
```


![png](output_11_0.png)


The MDS algorithm __recovers `ANY ONE` of the possible two-dimensional coordinate representations of our data__, using *only* the `N x N` distance matrix describing the relationship between the data points.

## MDS as Manifold Learning

__The usefulness of this becomes more apparent when we consider the fact that distance matrices can be computed from data in *any* dimension.__


So, for example, instead of simply rotating the data in the two-dimensional plane, we can project it into three dimensions:


```python
def random_projection(X, dimension, rseed=1):
    rng = np.random.RandomState(rseed)
    D = rng.randn(dimension, dimension)
    e, V = np.linalg.eigh(np.dot(D, D.T))
    return np.dot(X, V[:X.shape[1]])
    
X3 = random_projection(X, 3)
X3.shape
```




    (2035, 3)



Let's visualize these points to see what we're working with:


```python
from mpl_toolkits import mplot3d
ax = plt.axes(projection='3d')
ax.scatter3D(X3[:, 0], X3[:, 1], X3[:, 2],
             **colorize)

for tick in ax.w_xaxis.get_ticklines():
    tick.set_visible(False)
for tick in ax.w_yaxis.get_ticklines():
    tick.set_visible(False)
for tick in ax.w_zaxis.get_ticklines():
    tick.set_visible(False)
    
ax.view_init(azim=20, elev=10)
```


![png](output_16_0.png)


We can now `INVERSE THE PROCESS` 

- input this three-dimensional data
- compute the distance matrix
- then determine the optimal two-dimensional embedding for this distance matrix.

__The result recovers a representation of the original data:__


```python
model = MDS(n_components=2, random_state=1)
out3 = model.fit_transform(X3)
plt.scatter(out3[:, 0], out3[:, 1], **colorize)
plt.axis('equal');
```


![png](output_18_0.png)


__The output is exactly the same as when we input the `Distance Matrix` to the model fitting__


__This is essentially the goal of a manifold learning estimator: given high-dimensional embedded data, it seeks a low-dimensional representation of the data that ``preserves certain relationships within the data``.__


### In the case of MDS, the quantity preserved is the distance between every pair of points.

## Nonlinear Embeddings: Where MDS Fails

Where MDS breaks down is when the embedding is nonlinear


Consider the following embedding, which takes the input and contorts it into an "S" shape in three dimensions:


```python
def make_curve(X):
    t = (X[:, 0] - 1) * 0.9 * np.pi
    x = np.sin(t)
    y = X[:, 1]
    z = np.sign(t) * (np.cos(t) - 1)
    return np.vstack((x, y, z)).T

XC = make_curve(X)
```


```python
from mpl_toolkits import mplot3d
ax = plt.axes(projection='3d')
ax.scatter3D(XC[:, 0], XC[:, 1], XC[:, 2],
             **colorize);
for tick in ax.w_xaxis.get_ticklines():
    tick.set_visible(False)
for tick in ax.w_yaxis.get_ticklines():
    tick.set_visible(False)
for tick in ax.w_zaxis.get_ticklines():
    tick.set_visible(False)
    
ax.view_init(azim=80, elev=10)
```


![png](output_22_0.png)


The fundamental relationships between the data points are still there, but the data has been transformed in a nonlinear way: it has been wrapped-up into the shape of an "S."

If we try a simple MDS algorithm on this data, it is not able to "unwrap" this nonlinear embedding, and we lose track of the fundamental relationships in the embedded manifold:


```python
from sklearn.manifold import MDS
model = MDS(n_components=2, random_state=2)
outS = model.fit_transform(XC)
plt.scatter(outS[:, 0], outS[:, 1], **colorize)
plt.axis('equal');
```


![png](output_24_0.png)


Apparently MDS fails to unwrap the string back to its original state.

## Nonlinear Manifolds: Locally Linear Embedding

Since the source of the problem is that MDS tries to preserve distances between faraway points when constructing the embedding.

But what if we instead modified the algorithm such that it only preserves distances between nearby points? (geodesic(测地线))

- use KNN:


```python
from mpl_toolkits.mplot3d.art3d import Line3DCollection
from sklearn.neighbors import NearestNeighbors

# construct lines for MDS
rng = np.random.RandomState(1)
ind = rng.permutation(len(X))
lines_MDS = [(XC[i], XC[j]) for i in ind[:200] for j in ind[200:400]]

# construct lines for LLE
nbrs = NearestNeighbors(n_neighbors=200).fit(XC).kneighbors(XC[ind[:200]])[1]
lines_LLE = [(XC[ind[i]], XC[j]) for i in range(200) for j in nbrs[i]]
titles = ['MDS Linkages', 'LLE Linkages (200 NN)']
```


```python
fig, ax = plt.subplots(1, 2, figsize=(16, 6),
                       subplot_kw=dict(projection='3d', facecolor='none'))
fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0, wspace=0)

for axi, title, lines in zip(ax, titles, [lines_MDS, lines_LLE]):
    axi.scatter3D(XC[:, 0], XC[:, 1], XC[:, 2], **colorize);
    axi.add_collection(Line3DCollection(lines, lw=0.5, color='black',
                                        alpha=0.05))
    axi.view_init(elev=10, azim=80)
    axi.set_title(title, size=18)
```


![png](output_28_0.png)


__Each faint line represents a distance that should be preserved in the embedding.__

On the left is a representation of the model used by __MDS__: it tries to preserve the distances between __each pair__ of points in the dataset.

On the right is a representation of the model used by locally linear embedding (LLE): rather than preserving *all* distances, it instead tries to preserve __only the distances between *neighboring points*__: in this case, the nearest 200 neighbors of each point.

Now we can see why MDS fails: __there is no way to flatten this data while adequately preserving the length of every line drawn between the two points.__


But if we use LLE instead, __we could imagine unrolling the data in a way that keeps the lengths of the lines approximately the same.__


LLE comes in a number of versions; here we will use the __modified LLE__ algorithm to recover the embedded two-dimensional manifold.


In general, modified LLE does better than other versions of the algorithm at recovering well-defined manifolds with very little distortion:


```python
from sklearn.manifold import LocallyLinearEmbedding
model = LocallyLinearEmbedding(n_neighbors=200, n_components=2, method='modified',
                               eigen_solver='dense')
out = model.fit_transform(XC)

fig, ax = plt.subplots()
ax.scatter(out[:, 0], out[:, 1], **colorize)
ax.set_ylim(0.15, -0.15);
```


![png](output_30_0.png)


The result remains somewhat distorted compared to our original manifold, but captures the essential relationships in the data.

##  Manifold Methods Pros and Cons

In practice manifold learning techniques tend to be finicky enough that they are rarely used except for some simple qualitative visualization of high-dimensional data.

Cons:
- In manifold learning, there is __no good framework for handling missing data__. In contrast, there are straightforward iterative approaches for missing data in PCA.
- In manifold learning, the presence of __noise in the data can "short-circuit" the manifold and drastically change the embedding__. In contrast, PCA naturally filters noise from the most important components.
- The manifold embedding result is generally __highly dependent on the number of neighbors chosen, and there is generally no solid quantitative way to choose an optimal number of neighbors__. In contrast, PCA does not involve such a choice.
- In manifold learning, the __globally optimal number of output dimensions is difficult to determine__. In contrast, PCA lets you find the output dimension based on the explained variance.
- In manifold learning, the __meaning of the embedded dimensions is not always clear__. In PCA, the principal components have a very clear meaning.
- In manifold learning the __computational expense of manifold methods scales as O[N^2] or O[N^3]__. For PCA, there exist randomized approaches that are generally much faster 

With all these cons, the only clear advantage of manifold learning methods over PCA is their ability to preserve nonlinear relationships in the data; for this reason we should __explore data with manifold methods only after first exploring them with PCA.__

Scikit-Learn implements several common variants of manifold learning beyond Isomap and LLE:  

[for more see here](http://scikit-learn.org/stable/modules/manifold.html)


## Example: Isomap on Faces

One place manifold learning is often used is in understanding the relationship between high-dimensional data points.

A common case of high-dimensional data is images: 

for example, a set of images with 1,000 pixels each can be thought of as a collection of points in 1,000 dimensions 

- the brightness of each pixel in each image defines the coordinate in that dimension.




```python
from sklearn.datasets import fetch_lfw_people
faces = fetch_lfw_people(min_faces_per_person=60)
```


```python
fig, ax = plt.subplots(4, 8, subplot_kw=dict(xticks=[], yticks=[]))
for i, axi in enumerate(ax.flat):
    axi.imshow(faces.images[i], cmap='gray')
```


![png](output_36_0.png)


Next we would plot a low-dimensional embedding of the 2,914-dimensional data to learn the fundamental relationships between the images.

One useful way to start is to compute a PCA, and examine the explained variance ratio, which will give us an idea of how many linear features are required to describe the data:


```python
from sklearn.decomposition import PCA
model = PCA(100, svd_solver='randomized').fit(faces.data)
plt.plot(np.cumsum(model.explained_variance_ratio_))
plt.xlabel('n components')
plt.ylabel('cumulative variance');
```


![png](output_38_0.png)


We see that for this data, nearly 100 components are required to preserve 90% of the variance: this tells us that the data is intrinsically very high dimensional

__When this is the case, nonlinear manifold embeddings like LLE and Isomap can be helpful.__

We can compute an Isomap embedding on these faces using the same pattern shown before:


```python
from sklearn.manifold import Isomap
model = Isomap(n_components=2)
proj = model.fit_transform(faces.data)
proj.shape
```




    (1348, 2)




```python
from matplotlib import offsetbox

def plot_components(data, model, images=None, ax=None,
                    default_frac=0.05, cmap='gray'):
    ax = ax or plt.gca()
    
    proj = model.fit_transform(data)
    ax.plot(proj[:, 0], proj[:, 1], '.k')
    
    # plot the image the dot represented nearby
    if images is not None:
        min_dist_2 = (default_frac * max(proj.max(0) - proj.min(0))) ** 2
        shown_images = np.array([2 * proj.max(0)])
        for i in range(data.shape[0]):
            dist = np.sum((proj[i] - shown_images) ** 2, 1)
            if np.min(dist) < min_dist_2:
                # don't show points that are too close
                continue
            shown_images = np.vstack([shown_images, proj[i]])
            imagebox = offsetbox.AnnotationBbox(
                offsetbox.OffsetImage(images[i], cmap=cmap),
                                      proj[i])
            ax.add_artist(imagebox)
```


```python
fig, ax = plt.subplots()
plot_components(faces.data,
                model=Isomap(n_components=2),
                images=faces.images[:, ::2, ::2])
```


![png](output_42_0.png)


The result is interesting: 

__the first two Isomap dimensions seem to describe global image features: the overall darkness or lightness of the image from left to right, and the general orientation of the face from bottom to top.__
