{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "$$h_{\\theta}(x)=g(\\theta^T x)$$\n",
    "\n",
    "$$\\text{where    }g(z)=\\frac{1}{1+e^{-z}}$$\n",
    "\n",
    "$$\\text{thus   } h_{\\theta}(x)=\\frac{1}{1+e^{-\\theta^Tx}}$$\n",
    "\n",
    "\n",
    "\n",
    "### Interpretation of hypothesis output\n",
    "\n",
    "$$h_{\\theta}(x)=P(y=1 \\mid x;\\theta)$$\n",
    "\n",
    "$$\\text{probability that } y=1, \\text{given } x, \\text{parameterized by } \\theta$$\n",
    "\n",
    "$$\\text{predict  }y=1 \\text{  if  } h_{\\theta}(x) \\geq 0.5, \\text{      which means  } g(\\theta^T x) \\geq 0.5, \\text{  so  } \\theta^T x \\geq 0$$\n",
    "$$\\text{predict  }y=0 \\text{  if  } h_{\\theta}(x) \\leq 0.5  \\text{      which means  } g(\\theta^T x) \\leq 0.5, \\text{  so  } \\theta^T x \\leq 0$$\n",
    "\n",
    "\n",
    "### cost function\n",
    "$$Cost(h_{\\theta}(x), y)= \\left\\{\\begin{matrix} -\\log (h_{\\theta}(x)) \\:\\: \\text{ if } y=1\n",
    "\\\\ - \\log (1- h_{\\theta}(x)) \\text{ if } y=0\n",
    "\\end{matrix}\\right.$$\n",
    "\n",
    "#### if y=1\n",
    "![](../images/22917.png)\n",
    "\n",
    "![](../images/22918.png)\n",
    "\n",
    "\n",
    "$$Cost=0 \\text{  if  } y=1, h_{\\theta}(x)=1, h_{\\theta}(x) \\rightarrow 0 \\text{   Cost   } \\rightarrow \\infty$$\n",
    "\n",
    "#### if y=0\n",
    "\n",
    "![](../images/22919.png)\n",
    "\n",
    "![](../images/22920.png)\n",
    "\n",
    "\n",
    "![](../images/22915.png)\n",
    "\n",
    "![](../images/22916.png)\n",
    "\n",
    "### Cross-entropy (a more compact way to present the previous cost function)\n",
    "\n",
    "$$Cost(h_{\\theta}(x), y)=- \\log (h_{\\theta}(x)^{y} (1-h_{\\theta}(x))^{1-y})= -y \\log (h_{\\theta}(x))- (1-y) \\log (1-h_{\\theta}(x))$$\n",
    "\n",
    "\n",
    "__thus__\n",
    "\n",
    "$$Cost(h_{\\theta}(x), y)= \\left\\{\\begin{matrix} -\\log (h_{\\theta}(x)) \\:\\: \\text{ if } y=1\n",
    "\\\\ - \\log (1- h_{\\theta}(x)) \\text{ if } y=0\n",
    "\\end{matrix}\\right.$$\n",
    "\n",
    "\n",
    "$$J(\\theta)= \\frac{1}{m} \\sum^m_{i=1} Cost(h_{\\theta}(x^i), y)=\\frac{1}{m} \\sum^m_{i=1} - \\log (h_{\\theta}(x^i)^{y^i} (1-h_{\\theta}(x^i))^{1-y^i})$$\n",
    "\n",
    "$$ = \\frac{1}{m} \\sum^m_{i=1} -y^i \\log (h_{\\theta}(x^i))- (1-y^i) \\log (1-h_{\\theta}(x^i))$$\n",
    "\n",
    "$$ \\text{To fit parameters   } \\theta$$\n",
    "\n",
    "$$\\min_{\\theta} J(\\theta)$$\n",
    "\n",
    "$$\\text{To make a prediction given new x:   output }h_{\\theta}(x)=\\frac{1}{1+e^{-\\theta ^Tx}}$$\n",
    "\n",
    "\n",
    "__Gradient Descent__\n",
    "\n",
    "$$\\text{    repeat   } \\left \\{ \\theta_j := \\theta - \\alpha \\frac{\\partial}{\\partial \\theta_j} J(\\theta) \\right \\}$$\n",
    "\n",
    "$$\\text{    repeat   } \\left \\{ \\theta_j := \\theta - \\alpha \\sum^m_{i=1} (h_{\\theta}(x^i)-y^i) x_j^i \\right \\}$$\n",
    "\n",
    "\n",
    "![](../images/22921.png)\n",
    "\n",
    "$$h_{\\theta}^i(x)=P(y=i \\mid x;\\theta) \\: (i=1,2,3)$$\n",
    "\n",
    "$$\\text{Train a logistic regression classifier  } h_{\\theta}^i(x) \\text{    for each class i to predict the prob that  } y=i$$\n",
    "\n",
    "$$\\text{On a new input to-be tested x,to make a prediction, pick the class i which maximizes   } \\max_{i} h_{\\theta}^i(x)$$\n",
    "\n",
    "\n",
    "## Regularization for Linear Regression\n",
    "\n",
    "#### Overfitting: if we have too many features, the learned hypothesis may fit the training set too well, but fail to generalize to new examples:\n",
    "\n",
    "$$J(\\theta)=\\frac{1}{2m} \\sum^m_{i=1} (h_{\\theta}(x^i)-y^i)^2 \\approx 0$$\n",
    "\n",
    "![](../images/22922.png)\n",
    "\n",
    "> Solutions?\n",
    "\n",
    "- Reduce number of features:\n",
    "    - feature engineering\n",
    "    - model selection algorithm\n",
    "- Regularization\n",
    "    - keep all features, but reduce magnitudes of params (in our case above: theta)\n",
    "        - work well when there're lots of features, each contributes approximately equally to predicting y \n",
    "\n",
    "__After regularization__:\n",
    "\n",
    "$$J(\\theta)=\\frac{1}{2m} \\left [ \\sum ^m _{i=1} (h_{\\theta} (x^i) - y^i) ^ 2 + \\lambda \\sum^{n} _{j=1} \\theta_j ^2 \\right ] \\text{   ridge regularization here?  }$$\n",
    "\n",
    "\n",
    "- Notes:\n",
    "\n",
    "$$\\text{if } \\lambda \\text{   set to an extremely large value, say } \\lambda = 10^{10} \\text{ , then it tends to underfit, vice versa}$$\n",
    "\n",
    "\n",
    "__GD__:\n",
    "\n",
    "\n",
    "$$\\text{    Repeat   } \\left \\{ \\theta_j := \\theta_j - \\alpha \\left ( \\frac{1}{m} \\sum^m_{i=1} (h_{\\theta}(x^i)-y^i) x_j^i - \\frac{\\lambda}{m} \\theta_j \\right ) \\right \\}$$\n",
    "\n",
    "\n",
    "$$\\theta_j := \\theta_j (1- \\alpha \\frac{\\lambda}{m}) - \\frac{\\alpha}{m} \\sum ^m_{i=1} \\left ( h_{\\theta} (x^i) - y^i \\right ) x^i_j$$\n",
    "\n",
    "__Normal Equation__:\n",
    "\n",
    "\n",
    "$$\\begin{bmatrix}\n",
    "0 &  & & & \\\\ \n",
    " & 1 & & & \\\\\n",
    " &  & 1 & & \\\\ \n",
    " &  & & ... & \\\\ \n",
    " &  & &  & ... & \\\\ \n",
    " &  &  & & & 1\n",
    "\\end{bmatrix} \\subseteq \\mathbb{R}^{n+1}$$\n",
    "\n",
    "$$\\theta = \\left (x^T x + \\lambda  \\begin{bmatrix}\n",
    "0 &  & & & \\\\ \n",
    " & 1 & & & \\\\\n",
    " &  & 1 & & \\\\ \n",
    " &  & & ... & \\\\ \n",
    " &  & &  & ... & \\\\ \n",
    " &  &  & & & 1\n",
    "\\end{bmatrix} \\right ) ^{-1} x^T y$$\n",
    "\n",
    "\n",
    "### Attention!!\n",
    "\n",
    "$$\\text{After adding a regularization term, then    }x^T x + \\lambda  \\begin{bmatrix}\n",
    "0 &  & & & \\\\ \n",
    " & 1 & & & \\\\\n",
    " &  & 1 & & \\\\ \n",
    " &  & & ... & \\\\ \n",
    " &  & &  & ... & \\\\ \n",
    " &  &  & & & 1\n",
    "\\end{bmatrix} \\text{  is invertible}$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization for Logistic Regression\n",
    "\n",
    "$$ J(\\theta)= - \\left [ \\frac{1}{m} \\sum^m_{i=1} y^i \\log (h_{\\theta}(x^i))+ (1-y^i) \\log (1-h_{\\theta}(x^i)) \\right ] + \\frac{\\lambda}{2m} \\sum^{n}_{j=1} \\theta_j^2$$\n",
    "\n",
    "\n",
    "\n",
    "$$\\text{    Repeat   } \\left \\{ \\theta_j := \\theta_j - \\alpha \\left ( \\frac{1}{m} \\sum^m_{i=1} (h_{\\theta}(x^i)-y^i) x_j^i - \\frac{\\lambda}{m} \\theta_j \\right ) \\right \\}$$\n",
    "\n",
    "$$\\text{the difference from above is: for here   } h_{\\theta}(x)=\\frac{1}{1+e^{-\\theta^Tx}}$$\n",
    "\n",
    "\n",
    "$$\\frac{\\partial}{\\partial \\theta_0} J(\\theta)= \\frac{1}{m} \\sum^m_{i=1} (h_{\\theta}(x^i)-y^i) x_0^i$$\n",
    "\n",
    "$$\\frac{\\partial}{\\partial \\theta_1} J(\\theta)= \\frac{1}{m} \\sum^m_{i=1} (h_{\\theta}(x^i)-y^i) x_1^i - \\frac{\\lambda}{m} \\theta_1$$\n",
    "\n",
    "\n",
    "$$\\frac{\\partial}{\\partial \\theta_2} J(\\theta)= \\frac{1}{m} \\sum^m_{i=1} (h_{\\theta}(x^i)-y^i) x_2^i - \\frac{\\lambda}{m} \\theta_2$$\n",
    "\n",
    "$$\\text{.....}$$\n",
    "\n",
    "\n",
    "$$\\frac{\\partial}{\\partial \\theta_n} J(\\theta)= \\frac{1}{m} \\sum^m_{i=1} (h_{\\theta}(x^i)-y^i) x_n^i - \\frac{\\lambda}{m} \\theta_n$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
