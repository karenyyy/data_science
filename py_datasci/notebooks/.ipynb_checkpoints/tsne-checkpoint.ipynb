{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# t-SNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t-SNE is a new award winning technique for dimension reduction and data visualization. t-SNE not only captures the local structure of the higher dimension but also preserves the global structures of the data like clusters. It has stunning ability to produce well-defined segregated clusters. t-SNE is based on stochastic neighbor embedding(SNE). t-SNE was developed to address some of the problems in SNE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SNE\n",
    "For understanding t-SNE, we need to know about stochastic neighbor embedding and its shortcomings. Stochastic neighbor embedding uses probablistic approach to embedd a high dimension dataset into lower dimension by preserving the nieghborhood structure of the dataset. A gaussian probability distribution centered on each point is defined over all the potential neighbors of this point. SNE aims to minimize the difference in probablity distribution in higher dimension and lower dimension.  \n",
    "For each object, $i$ and it's neighbor $j$, we compute a $P_{i|j}$ which reflects the probability that $j$ is neighbor of $i$    \n",
    "$\\hspace{7em} P_{i|j} = \\frac{\\exp(-d_{ij}^2)}{\\Sigma_{k\\ne i}\\exp(-d_{ij}^2)})$ where  \n",
    "$\\hspace{7em} d_{ij}^2$ is the dissimilarity between element $i$ and $j$ given as input or calculated from the dataset provided.  \n",
    "The dissimilarity between $x_i$ and $x_j$ can be calculated using the following formula  \n",
    "$\\hspace{7em} d_{ij}^2 = \\frac{||x_i-x_j||^2}{2\\sigma_i^2}$, where $\\sigma_i$ generally calculated through a binary search by equating the entropy of the distribution centered at $x_i$ to perplexity which is chosen by hand. This method generates a probability matrix which is asymmetric.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, SNE does the same calculation for every $Y_i$ and $Y_j$ in the lower dimension with gaussian probability distribution with $\\sigma = 0.5$  \n",
    "$\\hspace{7em} q_{ij} = \\frac{\\exp(-||y_i-y_j||^2)}{\\Sigma_k \\exp(-||y_k-y_i||^2)}$  \n",
    "Now, SNE tries to minimize the difference between these two distributions. We can calculate the difference between two distributions using Kullback-Liebler divergence. For two discrete distirbution $P$ and $Q$ KL divergence is given by  \n",
    "$\\hspace{7em} D_{KL}(P||Q) = \\Sigma_i P_{i}\\frac{P_{i}}{Q_{i}}$.   \n",
    "SNE defines a cost function based of the difference between $p_{ij}$ and $q_{ij}$ which is given by  \n",
    "$\\hspace{7em} C = \\Sigma_i\\Sigma_j P_{ij}log(\\frac{P_{ij}}{q_{ij}})$  \n",
    "While embedding the dataset in lower dimension, two kinds of error can occur, first neighbors are mapped as faraway points($p_{ij}$ is large and  $q_{ij}$ is small) and points which are far away mapped as neighbors( $p_{ij}$ is small while $q_{ij}$ is large). Look closely at the cost function, the cost of the first kind of error i.e. mapping large $P_{ij}$ with small $q_{ij}$ is smaller than the cost while mapping small $p_{ij}$ as  large $q_{ij}$. The gradient to optimize the cost function is given by  \n",
    "$\\hspace{7em} \\frac{\\delta C}{\\delta Y} = 2\\Sigma_i (P_{ij}-q_{ij}+P_{ji}-q_{ji})(y_i - y_j)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the shorcomings of SNE approach are asymmetric probability matrix $P$, crowding problem. As pointed out earlier the probability matrix $P$ is asymmetric. Suppose a point $X_i$ is far away from other points, it's $P_{ij}$ will be very small for all $j$. So, It will have little effect on the cost function and embedding it correctly in the lower dimension will be hard. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any n-dimensional euclidean space can have an object with n+1 or less equidistant vertices not more than that. Now, when the intrinsic dimension of a dataset is high say 20, and we are reducing it's dimensions from 100 to 2 or 3 our solution will be affected by crowding problem. The amount of space availble to map close points in 10 or 15 dimension will always be greater than the space availble in 2 or 3 dimensions. In order to map close points properly, moderately distant points will be pushed too far. This will eat the gaps in original clusters and it will look like single giant cluster. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to brush up few more topics before we move to t-SNE.  \n",
    "**Student-t distribution** --  Student-t distribution is a continuous symmetric probability distribution function with heavy tails. It is has only one parameter *degree of freedom*. As the *degree of freedom* increases it approaches the normal distribution function.  When *degree of freedom* =1, it takes the form of cauchy distribution function and its probability density function is given by  \n",
    "$\\hspace{7em} f(t) = \\frac{1}{\\pi (1+t^2)}$  \n",
    "**Entropy** -- Entrophy is measure of the average information contained in a data. For a variable $x$ with $pdf$  $p(x)$  , it is given by  \n",
    "$\\hspace{7em} H(x) = - \\Sigma_{i}\\,(\\,p\\,(x_i) \\times  log_2(\\,p(\\,x_i\\,)))$  \n",
    "**Perpexility** -- In information theory , perplexity measures how good a probability distribution predicts a sample. A low perplexity indicates that distribution function is good at predicting sample. It is given by  \n",
    "$\\hspace{7em} Perpx(x) = 2^{H(x)}$, where $H(x)$ is the entropy of the distribution.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## t-SNE algorithm.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t-SNE differs from SNE in two ways, first it uses a student-t distribution to measure the similarity between points $Y_i$ and $Y_j$ in the lower dimension,secondly for the higher dimension it uses symmetric probability distribution such that $P_{ij} = P_{ji}$. Let's discuss the symmetric probability distribution first.  \n",
    "t-SNE defines the probability $P_{ij}$ as  \n",
    "$\\hspace{7em} P_{ij} = \\frac{P_{ij}+P_{ij}}{2n}$  \n",
    "This formulation makes sure that $\\Sigma_jP_{ij} \\gt \\frac{1}{2n}$ for every $x_i$ and $x_i$ makes a significant contribution to the cost function. The gradient defined above becomes much simpler now  \n",
    "$\\hspace{7em} \\frac{\\delta C}{\\delta Y_i} = 4 \\Sigma_{ij}(P_{ij}-q_{ij})(Y_i-Y_j)$   \n",
    "With student-t distribution having one degree one freedom , pairwise probability can be defined as  \n",
    "$\\hspace{7em} q_{ij} = \\frac{(1+||y_i-y_j||^2)^{-1}}{\\Sigma_{k\\ne i}(1+||y_i-y_k||^2)^{-1}}$  \n",
    "Now the gradient changes to   \n",
    "$\\hspace{7em}\\frac{\\delta C}{\\delta Y_i} = 4 \\Sigma_{ij}(P_{ij}-q_{ij})(Y_i-Y_j)(1+||y_i-y_j||^2)^{-1}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### psuedocode for t-SNE\n",
    "$\\text{begin}$\n",
    "1. compute pairwise similarity $P_{ij}$ for every $i$ and $j$.  \n",
    "2. Update the probability with $P_{ij}$ = $\\frac{P_{ij}+P_{ji}}{2n}$  \n",
    "3. choose a random solution $Y_0 = (y_0,y_1,y_2,...y_n)$ where $y_i \\in R^d$\n",
    "4. While not done:  \n",
    "   $\\hspace{1em}$compute pairwise similairites for $Y_0$  \n",
    "   $\\hspace{1em}$compute the gradient $\\frac {\\delta C}{\\delta y_i}$  \n",
    "   $\\hspace{1em}$update the solution $y_i^t = y_i^{t-1} + \\eta \\frac{\\delta C}{\\delta y_i} + (y_i^{t-1} - y_i^{t-2})$  \n",
    "   $\\hspace{1em}$if $t \\gt \\text{max_iter}$ :  \n",
    "   $\\hspace{2em}$break  \n",
    "   $\\hspace{1em}$else  \n",
    "   $\\hspace{2em}$t = t+1  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's implement the algorithm step by step\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute pariwise similarities\n",
    "For computing pairwise similarities we need to know the variance $\\sigma_i$ for the gaussian centered at $x_i$. \n",
    "One might think why not set a single value of $sigma_i$ for every $x_i$. The density of data is likely to vary, we need smaller $sigma_i$ for places with higher densities and bigger $\\sigma_i$ for places where points are far away. The entropy of the gaussian distribution centered at $x_i$ increases as $\\sigma_i$ increases. To get the $\\sigma_i$ we need to perform a binary search such that perplexity of the gaussian distribution centered at $x_i$ is equal to the perplexity specified by the user.  \n",
    "Now, If you are thinking how perplexity fits into all this. You can think of perplexity as the smooth measure for the number of neighbors.  \n",
    "Then, compute the pairwise similarity as    \n",
    "$\\hspace{7em} P_{i|j} = \\frac{\\exp(-d_{ij}^2)}{\\Sigma_{k\\ne i}\\exp(-d_{ij}^2)})$  \n",
    "Once all the pairwise similarities have been calculated, update the similarity using the following rule  \n",
    "$\\hspace{7em} P_{ii} = 0 $  \n",
    "$\\hspace{7em} P_{ij} = \\frac{P_{ij}+P_{ji}}{2n}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "data = datasets.load_digits(n_class=6)\n",
    "digits = data['data']\n",
    "target = data['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define simiality function\n",
    "from sklearn.metrics import pairwise as pw\n",
    "import numpy as np\n",
    "\n",
    "#Hbeta is the log of perplexity\n",
    "#instead of calculating sigma , we are calculating beta = 1/(2*sigma^2)\n",
    "\n",
    "def entropy(dist, beta):\n",
    "    \n",
    "    \n",
    "    p = np.exp(-dist*beta)\n",
    "    \n",
    "    sum_p = p.sum()\n",
    "    p = p/sum_p\n",
    "    \n",
    "    res = (p*np.log(p)).sum()\n",
    "    \n",
    "    return(-res, p)\n",
    "\n",
    "def calculate_pi(dists, perplexity,i):\n",
    "    \n",
    "    beta_min = float(\"-inf\")\n",
    "    beta_max = float(\"inf\")\n",
    "    log_u = np.log(perplexity)\n",
    "    dists = np.delete(dists,[i])\n",
    "    func = lambda x: entropy(dists, x)\n",
    "    \n",
    "    beta = 1.0\n",
    "    \n",
    "    f1,p = func(beta)\n",
    "    \n",
    "    iter = 0\n",
    "    diff = f1-log_u\n",
    "    while(True):\n",
    "        if np.abs(diff) < 1e-5:\n",
    "            \n",
    "            p = np.insert(p,i,0)\n",
    "            return(p)\n",
    "        \n",
    "        if diff > 0.0:\n",
    "        \n",
    "            beta_min = beta\n",
    "            if beta_max == np.inf or beta_max == -np.inf:\n",
    "                beta *= 2.0\n",
    "            else:\n",
    "                beta = (beta + beta_max)/2.0\n",
    "        else:\n",
    "            beta_max = beta\n",
    "            if beta_min == np.inf or beta_min == -np.inf:\n",
    "                beta /= 2.0\n",
    "    \n",
    "\n",
    "            else:\n",
    "                beta = (beta + beta_min) / 2.0\n",
    "        \n",
    "        iter += 1\n",
    "        f1,p = func(beta)\n",
    "        diff = f1-log_u\n",
    "        if iter > 50:\n",
    "            p = np.insert(p,i,0)\n",
    "            \n",
    "            return(p)\n",
    "\n",
    "def get_Pmat(X, perplexity):\n",
    "    n,p = X.shape\n",
    "    P = np.zeros((n,n))\n",
    "    D = pw.euclidean_distances(X, squared = True)\n",
    "    \n",
    "    for i in range(n):\n",
    "    \n",
    "        P[i] = calculate_pi(D[i], perplexity, i)\n",
    "\n",
    "    # update the probabilities\n",
    "    P = (P + P.T)\n",
    "    \n",
    "    P = P/P.sum()\n",
    "    return(P)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### calculation of low dimensional embedding\n",
    "Let's have a close look at the gradient obtained    \n",
    "$\\hspace{7em}\\frac{\\delta C}{\\delta y_i} = 4 \\Sigma_{ij}(P_{ij}-q_{ij})(y_i-y_j)(1+||y_i-y_j||^2)^{-1}$   \n",
    "The t-SNE gradient is better in two ways, first, it induces heavy cost when dissimilar points are modelled by small pairwise distances in lower dimension and secondly, though the cost induced in first case is high it doesn't approach infinity due to which separate clusters are not pushed very far from each other.  \n",
    "Update equation for $y_i$ is given as  \n",
    "$\\hspace{7em} y_i^t = y_i^{t-1} + \\eta \\frac{\\delta C}{\\delta y_i} + (y_i^{t-1} - y_i^{t-2})$  \n",
    "$\\hspace{7em}$ where $t$ is the iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a random sampled \n",
    "\n",
    "def q(D):\n",
    "    \n",
    "    D = 1/(1+D)\n",
    "    \n",
    "    np.fill_diagonal(D,0.0)\n",
    "    \n",
    "    q = D/D.sum()\n",
    "    \n",
    "    return(q)\n",
    "\n",
    "\n",
    "def get_gradient(Y,P,q,D_low):\n",
    "    \n",
    "    n, d = Y.shape\n",
    "    grad = np.zeros((n,d))\n",
    "    \n",
    "    D = 1/(1+D_low)\n",
    "    temp = (P - q)*D\n",
    "    \n",
    "    for i in range(n):\n",
    "        grad[i,:] = np.sum(temp[i,:].reshape(n,1)*(Y[i,:]-Y), axis=0)\n",
    "    \n",
    "    return(grad)\n",
    "\n",
    "\n",
    "def cost_KL(P,q):\n",
    "    \n",
    "    n = P.shape[0]\n",
    "    q[range(n),range(n)] = 1.00\n",
    "    temp = P/q\n",
    "    temp[temp==0.0]=1.0\n",
    "    cost = (P*np.log(temp)).sum()\n",
    "    return(cost)\n",
    "    \n",
    "\n",
    "np.random.seed(1000)\n",
    "\n",
    "def t_sne(X,perplexity, n_dim):\n",
    "    \n",
    "    P = get_Pmat(X, perplexity)\n",
    "    \n",
    "    # value of lower dimension\n",
    "    n,p = X.shape\n",
    "    \n",
    "    # generate a random sample of values for Y\n",
    "    Y = np.random.rand(n,n_dim)\n",
    "\n",
    "    # calculate the dissimilarity matrix for lower dimension points\n",
    "    D_low = pw.euclidean_distances(Y, squared = True)\n",
    "    minus = lambda x,y: (x-y).sum()\n",
    "    alpha = 0.5\n",
    "    max_iter=100\n",
    "\n",
    "    Y_t_1 = np.zeros( (n, n_dim) )\n",
    "    Y_t_2 = np.zeros( (n, n_dim) )\n",
    "    eta = 0.9\n",
    "    cost_old = 0.0\n",
    "    q_ = q(D_low)\n",
    "\n",
    "    cost_old = cost_KL(P,q_)\n",
    "    #print(\"init cost = \",cost_old)\n",
    "    \n",
    "    for i in range(max_iter):\n",
    "    \n",
    "        grad = get_gradient(Y,P,q_,D_low)\n",
    "    \n",
    "        diff =  Y_t_1 - Y_t_2 \n",
    "        Ynew = Y + eta*grad + alpha*(diff)\n",
    "        Y_t_2 = Y_t_1\n",
    "        Y_t_1 = Y\n",
    "        Y = Ynew\n",
    "    \n",
    "        D_low = pw.euclidean_distances(Y, squared = True)\n",
    "    \n",
    "        q_ = q(D_low)\n",
    "        cost_new = cost_KL(P,q_)\n",
    "        #print(\"iter = \",i, \"  cost = \",cost_new)\n",
    "        if abs(cost_old - cost_new) <1e-4:\n",
    "            break\n",
    "        else:\n",
    "            cost_old = cost_new\n",
    "\n",
    "    return(Y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to select perplexity for the t-sne?\n",
    "Perplexity is an important parameter of the t-sne algorithm. We will understand it's effects on the toy dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59c48bec78ff4692995a75b1ba8c4a09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>interactive</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in Jupyter Notebook or JupyterLab, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another notebook frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "interactive(children=(IntSlider(value=10, description='x', max=50, min=5, step=5), Output()), _dom_classes=('widget-interact',))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plotter>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "\n",
    "target = datasets.load_iris().target\n",
    "\n",
    "def plotter(x):\n",
    "    \n",
    "    Y = t_sne(iris, x,2)\n",
    "    setosa = Y[target==0]\n",
    "    versicolor = Y[target==1]\n",
    "    verginica = Y[target==2]\n",
    "    plt.scatter(setosa[:,0], setosa[:,1], c=\"b\",label=\"setosa\")\n",
    "    plt.scatter(versicolor[:,0], versicolor[:,1], c=\"g\",label=\"versicolor\")\n",
    "    plt.scatter(verginica[:,0], verginica[:,1], c=\"r\",label=\"verginica\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "interact(plotter, x= widgets.IntSlider(min=5, max=50, value=10, step=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you change value of perplexity from the slider you will see that, it's culsters changes dramatically. For t_sne to be meaningful we have to choose right value of perplexity. Perplexity balances the local and global aspects of the dataset. Very high value will lead to the merging of clusters into a single big clusters and low will produce many close small clusters which will be meaningless."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantages and Drawbacks\n",
    "\n",
    "t-SNE works well for non-linear datasets. It work much better than other non-linear algorithms. Problems arise when intrinsic dimensions are higher i.e. more than 2-3 dimensions. t-SNE has tendency to get stuck in local optima. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
