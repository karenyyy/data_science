{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def affine_forward(x, w, b):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    - x: A numpy array containing input data, of shape (N, d_1, ..., d_k)\n",
    "    - w: A numpy array of weights, of shape (D, M)\n",
    "    - b: A numpy array of biases, of shape (M,)\n",
    "    \"\"\"\n",
    "    # dimension\n",
    "    N = x.shape[0] # number of samples\n",
    "    D = np.prod(x.shape[1:]) # pix * pix * channels\n",
    "    x2 = np.reshape(x, (N, D)) \n",
    "    out = np.dot(x2, w) + b\n",
    "    cache = (x, w, b)\n",
    "    return out, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### backward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def affine_backward(dout, cache):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    - dout: Upstream derivative, of shape (N, M)\n",
    "    \"\"\"\n",
    "    x, w, b = cache\n",
    "    dx = np.dot(dout, w.T).reshape(x.shape)\n",
    "    dw = np.dot(x.reshape(x.shape[0], np.prod(x.shape[1:])).T, dout)\n",
    "    db = np.sum(dout, axis=0)\n",
    "    return dx, dw, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def relu_forward(x):\n",
    "    out = np.maximum(0, x)\n",
    "    cache = x\n",
    "    return out, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def relu_backward(dout, cache):\n",
    "    x = cache\n",
    "    dx = np.array(dout, copy=True)\n",
    "    dx[x <= 0] = 0\n",
    "    return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward pass for batch normalization.\n",
    "\n",
    "- During training the sample mean and (uncorrected) sample variance are computed from minibatch statistics and used to normalize the incoming data.\n",
    "\n",
    "- During training we also keep an exponentially decaying running mean of the mean\n",
    "    and variance of each feature, and these averages are used to normalize data\n",
    "    at test-time.\n",
    "    \n",
    "- At each timestep we update the running averages for mean and variance using\n",
    "    an exponential decay based on the momentum parameter:\n",
    "    - running_mean = momentum * running_mean + (1 - momentum) * sample_mean\n",
    "    - running_var = momentum * running_var + (1 - momentum) * sample_var\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchnorm_forward(x, gamma, beta, bn_param):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    - x: Data of shape (N, D)\n",
    "    - gamma: Scale parameter of shape (D,)\n",
    "    - beta: Shift paremeter of shape (D,)\n",
    "    - bn_param: \n",
    "      - mode: 'train' or 'test'; required\n",
    "      - eps: Constant for numeric stability\n",
    "      - momentum: Constant for running mean / variance.\n",
    "      - running_mean: Array of shape (D,) giving running mean of features\n",
    "      - running_var Array of shape (D,) giving running variance of features\n",
    "    Returns a tuple of:\n",
    "    - out: of shape (N, D)\n",
    "    - cache: A tuple of values needed in the backward pass\n",
    "    \"\"\"\n",
    "    mode = bn_param['mode']\n",
    "    eps = bn_param.get('eps', 1e-5)\n",
    "    momentum = bn_param.get('momentum', 0.9)\n",
    "\n",
    "    N, D = x.shape\n",
    "    # intialize running_mean and running_var to be 0s at first\n",
    "    running_mean = bn_param.get('running_mean', np.zeros(D, dtype=x.dtype))\n",
    "    running_var = bn_param.get('running_var', np.zeros(D, dtype=x.dtype))\n",
    "\n",
    "    out, cache = None, None\n",
    "    if mode == 'train':\n",
    "       \n",
    "        # Forward pass\n",
    "        # Step 1 - shape of mu (D,)\n",
    "        mu = 1 / float(N) * np.sum(x, axis=0) # mean of each column\n",
    "\n",
    "        # Step 2 - shape of var (N,D)\n",
    "        xmu = x - mu\n",
    "\n",
    "        # Step 3 - shape of carre (N,D)\n",
    "        carre = xmu**2\n",
    "\n",
    "        # Step 4 - shape of var (D,)\n",
    "        var = 1 / float(N) * np.sum(carre, axis=0)\n",
    "\n",
    "        # Step 5 - Shape sqrtvar (D,)\n",
    "        sqrtvar = np.sqrt(var + eps)\n",
    "\n",
    "        # Step 6 - Shape invvar (D,)\n",
    "        invvar = 1. / sqrtvar\n",
    "\n",
    "        # Step 7 - Shape va2 (N,D)\n",
    "        va2 = xmu * invvar\n",
    "\n",
    "        # Step 8 - Shape va3 (N,D)\n",
    "        va3 = gamma * va2\n",
    "\n",
    "        # Step 9 - Shape out (N,D)\n",
    "        out = va3 + beta\n",
    "\n",
    "        running_mean = momentum * running_mean + (1.0 - momentum) * mu\n",
    "        running_var = momentum * running_var + (1.0 - momentum) * var\n",
    "\n",
    "        cache = (mu, xmu, carre, var, sqrtvar, invvar,\n",
    "                 va2, va3, gamma, beta, x, bn_param)\n",
    "        \n",
    "    elif mode == 'test':\n",
    "        mu = running_mean\n",
    "        var = running_var\n",
    "        xhat = (x - mu) / np.sqrt(var + eps)\n",
    "        out = gamma * xhat + beta\n",
    "        cache = (mu, var, gamma, beta, bn_param)\n",
    "\n",
    "    else:\n",
    "        raise ValueError('Invalid forward batchnorm mode \"%s\"' % mode)\n",
    "\n",
    "    # Store the updated running means back into bn_param\n",
    "    bn_param['running_mean'] = running_mean\n",
    "    bn_param['running_var'] = running_var\n",
    "\n",
    "    return out, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchnorm_backward(dout, cache):\n",
    "    dx, dgamma, dbeta = None, None, None\n",
    "\n",
    "    mu, xmu, carre, var, sqrtvar, invvar, va2, va3, gamma, beta, x, bn_param = cache\n",
    "    eps = bn_param.get('eps', 1e-5)\n",
    "    N, D = dout.shape\n",
    "\n",
    "    dbeta = np.sum(dout, axis=0)\n",
    "    dgamma = np.sum((x - mu) * (var + eps)**(-1. / 2.) * dout, axis=0)\n",
    "    dx = (1. / N) * gamma * (var + eps)**(-1. / 2.) * (N * dout - np.sum(dout, axis=0)\n",
    "                                                       - (x - mu) * (var + eps)**(-1.0) * np.sum(dout * (x - mu), axis=0))\n",
    "    return dx, dgamma, dbeta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropout_forward(x, dropout_param):\n",
    "    p, mode = dropout_param['p'], dropout_param['mode']\n",
    "    if 'seed' in dropout_param:\n",
    "        np.random.seed(dropout_param['seed'])\n",
    "\n",
    "    mask = None\n",
    "    out = None\n",
    "\n",
    "    if mode == 'train':\n",
    "        mask = (np.random.rand(*x.shape) < p) / p\n",
    "        out = x * mask\n",
    "\n",
    "    elif mode == 'test':\n",
    "        mask = None\n",
    "        out = x\n",
    "\n",
    "    cache = (dropout_param, mask)\n",
    "    out = out.astype(x.dtype, copy=False)\n",
    "    return out, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropout_backward(dout, cache):\n",
    "    dropout_param, mask = cache\n",
    "    mode = dropout_param['mode']\n",
    "\n",
    "    dx = None\n",
    "    if mode == 'train':\n",
    "        dx = dout * mask\n",
    "\n",
    "    elif mode == 'test':\n",
    "        dx = dout\n",
    "    return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_forward_naive(x, w, b, conv_param):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    - x: Input data of shape (N, C, H, W)\n",
    "    - w: Filter weights of shape (F, C, HH, WW)\n",
    "    - b: Biases, of shape (F,)\n",
    "    - conv_param: A dictionary with the following keys:\n",
    "      - 'stride': The number of pixels between adjacent receptive fields in the\n",
    "        horizontal and vertical directions.\n",
    "      - 'pad': The number of pixels that will be used to zero-pad the input.\n",
    "    Returns a tuple of:\n",
    "    - out: Output data, of shape (N, F, H', W') where H' and W' are given by\n",
    "      H' = 1 + (H + 2 * pad - HH) / stride\n",
    "      W' = 1 + (W + 2 * pad - WW) / stride\n",
    "    - cache: (x, w, b, conv_param)\n",
    "    \"\"\"\n",
    "    out = None\n",
    "    \n",
    "    N, C, H, W = x.shape\n",
    "    F, C, HH, WW = w.shape\n",
    "    S = conv_param['stride']\n",
    "    P = conv_param['pad']\n",
    "\n",
    "    # Add padding to each image\n",
    "    x_pad = np.pad(x, ((0,), (0,), (P,), (P,)), 'constant')\n",
    "    # Size of the output\n",
    "    Hh = 1 + (H + 2 * P - HH) / S\n",
    "    Hw = 1 + (W + 2 * P - WW) / S\n",
    "\n",
    "    out = np.zeros((N, F, Hh, Hw))\n",
    "\n",
    "    for n in range(N):  # First, iterate over all the images\n",
    "        for f in range(F):  # Second, iterate over all the kernels\n",
    "            for k in range(Hh):\n",
    "                for l in range(Hw):\n",
    "                    out[n, f, k, l] = np.sum(\n",
    "                        x_pad[n, :, k * S:k * S + HH, l * S:l * S + WW] * w[f, :]) + b[f]\n",
    "\n",
    "    cache = (x, w, b, conv_param)\n",
    "    return out, cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
