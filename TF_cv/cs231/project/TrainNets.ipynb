{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute mean image. This takes a while to run on CPU.\n",
    "# http://stackoverflow.com/questions/7762948/how-to-convert-an-rgb-image-to-numpy-array\n",
    "\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "TRAIN_DIR = \"data/images/train/\"\n",
    "\n",
    "# We use Knuth's method for computing running mean, sdev\n",
    "m, S, n = 0, 0, 0\n",
    "\n",
    "subdirs = [d for d in listdir(TRAIN_DIR) if not isfile(join(TRAIN_DIR, d))]\n",
    "for i in range(len(subdirs)):\n",
    "    d = subdirs[i]\n",
    "    files = [f for f in listdir(join(TRAIN_DIR, d)) if isfile(join(TRAIN_DIR, d, f))]\n",
    "    print(\"Processing: \" + d + \" \" + str(i) + \"/\" + str(len(subdirs)))\n",
    "    for f in files:\n",
    "        file_path = join(TRAIN_DIR, d, f)\n",
    "        img = Image.open(file_path)\n",
    "        img.load()\n",
    "        data = np.asarray(img, dtype=\"int32\")\n",
    "        \n",
    "        x = np.average(data, axis=(0,1))\n",
    "        n = n + 1\n",
    "        m_prev = m\n",
    "        m = m + (x - m) / n\n",
    "        S = S + (x - m) * (x - m_prev)\n",
    "\n",
    "mean = m\n",
    "sdev = np.sqrt(S/(n-1))\n",
    "print(\"Mean: \" + str(mean))\n",
    "print(\"Sdev: \" + str(sdev))\n",
    "print(\"Tensor mean: \" + str(tensor_mean))\n",
    "print(\"Tensor sdev: \" + str(tensor_sdev))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import sampler\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1276\n",
      "125\n",
      "123\n",
      "Datasets loaded.\n"
     ]
    }
   ],
   "source": [
    "from dataset import AttnDataset\n",
    "\n",
    "TRAIN_DIR = \"data/images/train/\"\n",
    "VAL_DIR = \"data/images/val/\"\n",
    "TEST_DIR = \"data/images/test/\"\n",
    "BBOX_DIR = \"data/bbox/json/\"\n",
    "\n",
    "HOLDOUT_TRAIN_DIR = \"data/images/holdout/train/\"\n",
    "HOLDOUT_VAL_DIR = \"data/images/holdout/val/\"\n",
    "HOLDOUT_TEST_DIR = \"data/images/holdout/test/\"\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Computed sample mean, sdev\n",
    "mean = [0.48678957, 0.46590506, 0.41864723]\n",
    "sdev = [0.15854293, 0.15514862, 0.18052906]\n",
    "\n",
    "# ImageNet mean, sdev\n",
    "# mean = [0.485, 0.456, 0.406]\n",
    "# sdev = [0.229, 0.224, 0.225]\n",
    "\n",
    "# https://github.com/pytorch/examples/blob/master/imagenet/main.py\n",
    "\n",
    "TRAIN_TRANSFORMS = transforms.Compose([\n",
    "        transforms.RandomSizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=mean, std=sdev)\n",
    "    ])\n",
    "VAL_TEST_TRANSFORMS = transforms.Compose([\n",
    "        transforms.Scale(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=mean, std=sdev)\n",
    "    ])\n",
    "\n",
    "train_data = AttnDataset(TRAIN_DIR, BBOX_DIR, transform=TRAIN_TRANSFORMS)\n",
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "val_data = AttnDataset(VAL_DIR, BBOX_DIR, transform=VAL_TEST_TRANSFORMS)\n",
    "val_loader = DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "test_data = AttnDataset(TEST_DIR, BBOX_DIR, transform=VAL_TEST_TRANSFORMS)\n",
    "test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "holdout_train_data = AttnDataset(HOLDOUT_TRAIN_DIR, BBOX_DIR, transform=TRAIN_TRANSFORMS)\n",
    "holdout_train_loader = DataLoader(holdout_train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "holdout_val_data = AttnDataset(HOLDOUT_VAL_DIR, BBOX_DIR, transform=VAL_TEST_TRANSFORMS)\n",
    "holdout_val_loader = DataLoader(holdout_val_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "holdout_test_data = AttnDataset(HOLDOUT_TEST_DIR, BBOX_DIR, transform=VAL_TEST_TRANSFORMS)\n",
    "holdout_test_loader = DataLoader(holdout_test_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# Number of expected minibatches\n",
    "print(len(train_loader)) # Equivalent to print(len(train_data) / BATCH_SIZE)\n",
    "print(len(val_loader))\n",
    "print(len(test_loader))\n",
    "print(\"Datasets loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for minibatch, (x, y) in enumerate(train_loader):\n",
    "    x_var = Variable(x.type(torch.cuda.FloatTensor))\n",
    "    y_var = Variable(y.type(torch.cuda.LongTensor))\n",
    "    print(minibatch)\n",
    "    # print(x_var.size())\n",
    "    # print(y_var)\n",
    "    # labels = y_var[:,0]\n",
    "    # bounds = y_var[:,1:3]\n",
    "    # bboxes = y_var[:,3:]\n",
    "    # print(labels)\n",
    "    # print(bounds)\n",
    "    # print(boxes)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Training loop for host model\n",
    "\n",
    "from datetime import datetime\n",
    "from time import strftime\n",
    "\n",
    "def train_host_model(model, train_loader, val_loader, \n",
    "               checkpoints_path, log_path, lr=1e-2, start_epoch=0, session_id=0):\n",
    "        \n",
    "    NUM_EPOCHS = 50\n",
    "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n",
    "    epoch = start_epoch\n",
    "    \n",
    "    while epoch < NUM_EPOCHS:\n",
    "        \n",
    "        print(\"Epoch \" + str(epoch))\n",
    "        \n",
    "        # Train\n",
    "        print(\"  Train\")\n",
    "        model.train()\n",
    "        for minibatch, (x, y) in enumerate(train_loader):\n",
    "            \n",
    "            x_var = Variable(x.type(torch.cuda.FloatTensor))\n",
    "            labels = Variable(y[:,0].type(torch.cuda.LongTensor))\n",
    "            \n",
    "            scores = model.forward(x_var)\n",
    "            loss_fn = torch.nn.CrossEntropyLoss()\n",
    "            loss = loss_fn(scores, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if minibatch % 10 == 0:\n",
    "                print(\"      \" + str(minibatch) + \" \" + str(float(loss.data.cpu().numpy())))\n",
    "            \n",
    "        \n",
    "        print(\"  Eval\")\n",
    "        num_correct = torch.zeros(1).type(torch.cuda.FloatTensor)\n",
    "        num_samples = torch.zeros(1).type(torch.cuda.FloatTensor)\n",
    "        model.eval()\n",
    "        for minibatch, (x, y) in enumerate(val_loader):\n",
    "            \n",
    "            x_var = Variable(x.type(torch.cuda.FloatTensor), volatile=True)\n",
    "            y = y.type(torch.cuda.LongTensor)\n",
    "\n",
    "            scores = model.forward(x_var)\n",
    "            _, preds = scores.data.max(1) # argmax\n",
    "\n",
    "            num_correct += (preds == y[:,0]).sum()\n",
    "            num_samples += preds.size(0)\n",
    "            val_acc = (float(num_correct.cpu().numpy()) / num_samples.cpu().numpy())[0]\n",
    "\n",
    "            if minibatch % 10 == 0:\n",
    "                print(\"      \" + str(minibatch) + \" \" + str(val_acc))\n",
    "        \n",
    "        sid = str(session_id)\n",
    "        timenow = str(datetime.now().time())\n",
    "        \n",
    "        with open(log_path + sid + \".log\", 'a') as f:\n",
    "            f.write(\"\".join([sid, \"\\t\",\n",
    "                        str(epoch), \"\\t\",\n",
    "                        str(val_acc), \"\\t\",\n",
    "                        timenow, \"\\n\"]))\n",
    "        \n",
    "        torch.save(model.state_dict(), checkpoints_path + \"session_\" + sid + \"_epoch_\" + str(epoch) + \".checkpoint\")\n",
    "        \n",
    "        epoch += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### RESNET\n",
    "\n",
    "from models import ResnetHost\n",
    "\n",
    "# Define model and verify output size\n",
    "N_TRAIN_CLASSES = 160\n",
    "N_TEST_CLASSES = 38\n",
    "\n",
    "train_host_resnet = ResnetHost(n_classes=N_TRAIN_CLASSES)\n",
    "train_host_resnet.cuda()\n",
    "train_host_resnet.set_retrain([\"conv5_x\", \"avg_pool\", \"fc\"], True)\n",
    "\n",
    "test_host_resnet = ResnetHost(n_classes=N_TEST_CLASSES)\n",
    "test_host_resnet.cuda()\n",
    "test_host_resnet.set_retrain([\"conv5_x\", \"avg_pool\", \"fc\"], True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 80% top-1 validation accuracy (160-way)\n",
    "train_host_resnet.load_state_dict(torch.load(\"data/models/train_resnet_session_0_epoch_10.checkpoint\"))\n",
    "train_host_model(train_host_resnet, train_loader, val_loader,\n",
    "                \"data/models/train_resnet_\", \"data/logs/\",\n",
    "                lr=1e-3, start_epoch=11, session_id=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 83.6% top-1 validation accuracy (38-way)\n",
    "test_host_resnet.load_state_dict(torch.load(\"data/models/test_resnet_session_0_epoch_16.checkpoint\"))\n",
    "train_host_model(test_host_resnet, holdout_train_loader, holdout_val_loader,\n",
    "                \"data/models/test_resnet_\", \"data/logs/\",\n",
    "                lr=1e-3, start_epoch=17, session_id=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### VGG\n",
    "\n",
    "from models import ModVggHost\n",
    "\n",
    "# Define model and verify output size\n",
    "N_TRAIN_CLASSES = 160\n",
    "N_TEST_CLASSES = 38\n",
    "\n",
    "train_host_vggnet = ModVggHost(n_classes=N_TRAIN_CLASSES)\n",
    "train_host_vggnet.cuda()\n",
    "train_host_vggnet.set_retrain([\"bn2\", \"feats3\", \"bn3\", \"feats4\", \"bn4\", \"feats5\", \"bn5\", \"last\"], True)\n",
    "\n",
    "test_host_vggnet = ModVggHost(n_classes=N_TEST_CLASSES)\n",
    "test_host_vggnet.cuda()\n",
    "train_host_vggnet.set_retrain([\"bn2\", \"feats3\", \"bn3\", \"feats4\", \"bn4\", \"feats5\", \"bn5\", \"last\"], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "  Train\n",
      "      0 5.069231033325195\n",
      "      10 4.877032279968262\n",
      "      20 4.938404560089111\n",
      "      30 5.020724296569824\n",
      "      40 4.909976482391357\n",
      "      50 5.062371253967285\n",
      "      60 4.909876823425293\n",
      "      70 4.970588684082031\n",
      "      80 4.9219136238098145\n",
      "      90 4.841250419616699\n",
      "      100 4.859414100646973\n",
      "      110 4.740114212036133\n",
      "      120 4.833495616912842\n",
      "      130 4.836550235748291\n",
      "      140 4.946642875671387\n",
      "      150 4.869873046875\n",
      "      160 4.816774845123291\n",
      "      170 4.744311332702637\n",
      "      180 4.759790897369385\n",
      "      190 4.6575140953063965\n",
      "      200 4.638246059417725\n",
      "      210 4.815683841705322\n",
      "      220 4.856706619262695\n",
      "      230 4.796492099761963\n",
      "      240 5.200757026672363\n",
      "      250 4.7848968505859375\n",
      "      260 4.512701511383057\n",
      "      270 4.5988993644714355\n",
      "      280 4.4423017501831055\n",
      "      290 4.7351393699646\n",
      "      300 4.641568660736084\n",
      "      310 4.798869609832764\n",
      "      320 4.681436061859131\n",
      "      330 4.615627288818359\n",
      "      340 4.6568427085876465\n",
      "      350 4.693728446960449\n",
      "      360 4.789748668670654\n",
      "      370 4.724301815032959\n",
      "      380 4.498616695404053\n",
      "      390 4.558801651000977\n",
      "      400 4.342398166656494\n",
      "      410 4.217827320098877\n",
      "      420 4.486284255981445\n",
      "      430 4.718433856964111\n",
      "      440 4.257989883422852\n",
      "      450 4.630517959594727\n",
      "      460 4.651345252990723\n",
      "      470 4.754148960113525\n",
      "      480 4.564957141876221\n",
      "      490 4.141298770904541\n",
      "      500 4.656523704528809\n",
      "      510 4.252098083496094\n",
      "      520 4.451937675476074\n",
      "      530 4.442266941070557\n",
      "      540 4.3784918785095215\n",
      "      550 4.568843841552734\n",
      "      560 4.426888465881348\n",
      "      570 4.115472316741943\n",
      "      580 4.587362766265869\n",
      "      590 4.392958641052246\n",
      "      600 4.598443508148193\n",
      "      610 4.399295330047607\n",
      "      620 4.402502536773682\n",
      "      630 4.128505706787109\n",
      "      640 4.186026573181152\n",
      "      650 4.50758695602417\n",
      "      660 4.504565715789795\n",
      "      670 4.263593673706055\n",
      "      680 4.320553302764893\n",
      "      690 4.263571739196777\n",
      "      700 4.263103008270264\n",
      "      710 3.970047950744629\n",
      "      720 4.440202236175537\n",
      "      730 4.055606842041016\n",
      "      740 4.393523216247559\n",
      "      750 4.185410022735596\n",
      "      760 4.4243059158325195\n",
      "      770 4.083212852478027\n",
      "      780 4.114785194396973\n",
      "      790 3.9036974906921387\n",
      "      800 4.3371405601501465\n",
      "      810 4.3293352127075195\n",
      "      820 3.965276002883911\n",
      "      830 3.7698376178741455\n",
      "      840 3.9801878929138184\n",
      "      850 4.091296195983887\n",
      "      860 4.031080722808838\n",
      "      870 4.415866374969482\n",
      "      880 4.020928859710693\n",
      "      890 4.39691162109375\n",
      "      900 4.035735130310059\n",
      "      910 3.8830301761627197\n",
      "      920 4.130964279174805\n",
      "      930 4.1881842613220215\n",
      "      940 3.7574994564056396\n",
      "      950 3.8118460178375244\n",
      "      960 4.117884635925293\n",
      "      970 4.026031017303467\n",
      "      980 4.3390278816223145\n",
      "      990 3.998225212097168\n",
      "      1000 4.398013591766357\n",
      "      1010 4.106311798095703\n",
      "      1020 4.08036994934082\n",
      "      1030 4.2462968826293945\n",
      "      1040 4.155629634857178\n",
      "      1050 4.203073978424072\n",
      "      1060 3.8328652381896973\n",
      "      1070 3.901644468307495\n",
      "      1080 4.2053680419921875\n",
      "      1090 4.291845321655273\n",
      "      1100 3.8960635662078857\n",
      "      1110 3.9546284675598145\n",
      "      1120 3.8132450580596924\n",
      "      1130 3.741935968399048\n",
      "      1140 3.97668194770813\n",
      "      1150 3.892120122909546\n",
      "      1160 3.8102359771728516\n",
      "      1170 4.5197434425354\n",
      "      1180 3.8261630535125732\n",
      "      1190 3.963834762573242\n",
      "      1200 3.886280059814453\n",
      "      1210 3.979779005050659\n",
      "      1220 3.8090548515319824\n",
      "      1230 4.1037445068359375\n",
      "      1240 4.191402912139893\n",
      "      1250 4.146568298339844\n",
      "      1260 4.014710426330566\n",
      "      1270 3.981614589691162\n",
      "  Eval\n",
      "      0 0.15625\n",
      "      10 0.12642\n",
      "      20 0.12872\n",
      "      30 0.12248\n",
      "      40 0.124619\n",
      "      50 0.122243\n",
      "      60 0.117572\n",
      "      70 0.117958\n",
      "      80 0.118056\n",
      "      90 0.120707\n",
      "      100 0.122834\n",
      "      110 0.124718\n",
      "      120 0.124742\n",
      "Epoch 1\n",
      "  Train\n",
      "      0 3.830004930496216\n",
      "      10 3.8353419303894043\n",
      "      20 4.2709736824035645\n",
      "      30 4.095719814300537\n",
      "      40 4.0166544914245605\n",
      "      50 4.10481071472168\n",
      "      60 4.332825183868408\n",
      "      70 3.8937551975250244\n",
      "      80 3.868788003921509\n",
      "      90 3.846782922744751\n",
      "      100 4.292644500732422\n",
      "      110 3.931896209716797\n",
      "      120 3.9063034057617188\n",
      "      130 4.195312976837158\n",
      "      140 4.091590404510498\n",
      "      150 3.823932647705078\n",
      "      160 4.023261070251465\n",
      "      170 3.9243111610412598\n",
      "      180 3.815951347351074\n",
      "      190 4.006430625915527\n",
      "      200 3.7583694458007812\n",
      "      210 4.026939392089844\n",
      "      220 3.8440635204315186\n",
      "      230 4.167264938354492\n",
      "      240 4.100301265716553\n",
      "      250 3.9760775566101074\n",
      "      260 3.7773523330688477\n",
      "      270 4.3288798332214355\n",
      "      280 3.942185163497925\n",
      "      290 4.010459899902344\n",
      "      300 4.100199222564697\n",
      "      310 3.718538284301758\n",
      "      320 3.8457579612731934\n",
      "      330 3.7550902366638184\n",
      "      340 3.9746344089508057\n",
      "      350 3.868262529373169\n",
      "      360 4.024816036224365\n",
      "      370 3.7384307384490967\n",
      "      380 3.672475576400757\n",
      "      390 3.8553123474121094\n",
      "      400 3.6094398498535156\n",
      "      410 3.7371551990509033\n",
      "      420 3.935588836669922\n",
      "      430 3.726315975189209\n",
      "      440 3.676318883895874\n",
      "      450 4.116941452026367\n",
      "      460 3.7192482948303223\n",
      "      470 3.592829465866089\n",
      "      480 3.689685344696045\n",
      "      490 3.768465042114258\n",
      "      500 4.0013909339904785\n",
      "      510 3.776064395904541\n",
      "      520 3.5426113605499268\n",
      "      530 3.5643527507781982\n",
      "      540 3.880216121673584\n",
      "      550 3.617948055267334\n",
      "      560 4.164276123046875\n",
      "      570 4.049333572387695\n",
      "      580 4.0537614822387695\n",
      "      590 3.973619222640991\n",
      "      600 3.563944101333618\n",
      "      610 4.071497440338135\n",
      "      620 4.504836082458496\n",
      "      630 3.7217085361480713\n",
      "      640 3.587906837463379\n",
      "      650 3.8207802772521973\n",
      "      660 3.6838932037353516\n",
      "      670 3.7415523529052734\n",
      "      680 3.7123262882232666\n",
      "      690 4.296719074249268\n",
      "      700 3.708303689956665\n",
      "      710 3.632354497909546\n",
      "      720 3.830655574798584\n",
      "      730 3.657482862472534\n",
      "      740 3.9552648067474365\n",
      "      750 3.8217639923095703\n",
      "      760 4.001818656921387\n",
      "      770 3.8859262466430664\n",
      "      780 3.7895073890686035\n",
      "      790 4.053397178649902\n",
      "      800 3.742966413497925\n",
      "      810 4.077641010284424\n",
      "      820 3.7314271926879883\n",
      "      830 3.7114148139953613\n",
      "      840 3.671466827392578\n",
      "      850 3.393432378768921\n",
      "      860 4.096374034881592\n",
      "      870 3.5168263912200928\n",
      "      880 3.549311876296997\n",
      "      890 3.6252591609954834\n",
      "      900 3.580315589904785\n",
      "      910 3.7798714637756348\n",
      "      920 3.413794994354248\n",
      "      930 4.103489398956299\n",
      "      940 3.5682144165039062\n",
      "      950 4.101597309112549\n",
      "      960 3.575627088546753\n",
      "      970 3.5244481563568115\n",
      "      980 4.019888877868652\n",
      "      990 3.4040679931640625\n",
      "      1000 3.3450844287872314\n",
      "      1010 3.9263205528259277\n",
      "      1020 4.0345282554626465\n",
      "      1030 3.763728141784668\n",
      "      1040 3.8219077587127686\n",
      "      1050 3.933455467224121\n",
      "      1060 3.675551652908325\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-f2851555ce72>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m train_host_model(train_host_vggnet, train_loader, val_loader,\n\u001b[0;32m      3\u001b[0m                 \u001b[1;34m\"data/models/train_vggnet_\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"data/logs/\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m                 lr=1e-3, start_epoch=0, session_id=0)\n\u001b[0m",
      "\u001b[1;32m<ipython-input-3-ff28bd70940f>\u001b[0m in \u001b[0;36mtrain_host_model\u001b[1;34m(model, train_loader, val_loader, checkpoints_path, log_path, lr, start_epoch, session_id)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/cs231n/myVE35/lib/python3.5/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_variables)\u001b[0m\n\u001b[0;32m    144\u001b[0m                     'or with gradient w.r.t. the variable')\n\u001b[0;32m    145\u001b[0m             \u001b[0mgradient\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize_as_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfill_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 146\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_execution_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_backward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    147\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train_host_vggnet.load_state_dict(torch.load(\"data/models/train_vggnet_session_0_epoch_10.checkpoint\"))\n",
    "train_host_model(train_host_vggnet, train_loader, val_loader,\n",
    "                \"data/models/train_vggnet_\", \"data/logs/\",\n",
    "                lr=1e-3, start_epoch=0, session_id=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train_host_vggnet.load_state_dict(torch.load(\"data/models/train_vggnet_session_0_epoch_10.checkpoint\"))\n",
    "train_host_model(train_host_vggnet, holdout_train_loader, holdout_val_loader,\n",
    "                \"data/models/train_vggnet_\", \"data/logs/\",\n",
    "                lr=1e-2, start_epoch=0, session_id=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets loaded.\n"
     ]
    }
   ],
   "source": [
    "from dataset import AttnDataset\n",
    "\n",
    "TRAIN_DIR = \"data/images/train/\"\n",
    "VAL_DIR = \"data/images/val/\"\n",
    "TEST_DIR = \"data/images/test/\"\n",
    "BBOX_DIR = \"data/bbox/json/\"\n",
    "\n",
    "HOLDOUT_TRAIN_DIR = \"data/images/holdout/train/\"\n",
    "HOLDOUT_VAL_DIR = \"data/images/holdout/val/\"\n",
    "HOLDOUT_TEST_DIR = \"data/images/holdout/test/\"\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "mean = [0.48678957, 0.46590506, 0.41864723]\n",
    "sdev = [0.15854293, 0.15514862, 0.18052906]\n",
    "arrmean = np.array(mean)\n",
    "arrsdev = np.array(sdev)\n",
    "\n",
    "IMG_TRANSFORMS = transforms.Compose([\n",
    "        transforms.Scale(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=mean, std=sdev)\n",
    "    ])\n",
    "\n",
    "REVERSE_IMG_TRANSFORMS = transforms.Compose([\n",
    "        transforms.Normalize(mean=[0, 0, 0], std=(1.0 / arrsdev).tolist()),\n",
    "        transforms.Normalize(mean=(-arrmean).tolist(), std=[1, 1, 1]),\n",
    "        # Can't reverse crops or scaling as the information is lost\n",
    "        transforms.ToPILImage(),\n",
    "    ])\n",
    "\n",
    "def BBOX_TRANSFORMS(y):\n",
    "    synsetid, xmax, ymax = y[0], y[1], y[2] # width, height\n",
    "    x1, y1, x2, y2 = y[3], y[4], y[5], y[6]\n",
    "    \n",
    "    # Scale 256: https://github.com/pytorch/vision/blob/master/torchvision/transforms.py\n",
    "    \n",
    "    if (xmax <= ymax and xmax == 256) or (ymax <= xmax and ymax == 256):\n",
    "        pass\n",
    "    elif xmax < ymax:\n",
    "        ymax_new = 256.0 * ymax / xmax\n",
    "        y1 = int(y1 * (ymax_new / ymax))\n",
    "        y2 = int(y2 * (ymax_new / ymax))\n",
    "        ymax = int(ymax_new)\n",
    "        xmax_new = 256.0\n",
    "        x1 = int(x1 * (xmax_new / xmax))\n",
    "        x2 = int(x2 * (xmax_new / xmax))\n",
    "        xmax = int(xmax_new)\n",
    "    else:\n",
    "        xmax_new = 256.0 * xmax / ymax\n",
    "        x1 = int(x1 * (xmax_new / xmax))\n",
    "        x2 = int(x2 * (xmax_new / xmax))\n",
    "        xmax = int(xmax_new)\n",
    "        ymax_new = 256.0\n",
    "        y1 = int(y1 * (ymax_new / ymax))\n",
    "        y2 = int(y2 * (ymax_new / ymax))\n",
    "        ymax = int(ymax_new)\n",
    "\n",
    "    # CenterCrop 224\n",
    "    \n",
    "    cx1 = int(round((xmax-224)) / 2.0) # Get crop coordinates\n",
    "    cy1 = int(round((ymax-224)) / 2.0)\n",
    "    cx2 = cx1 + 224\n",
    "    cy2 = cy1 + 224\n",
    "    \n",
    "    x1 = min(max(x1, cx1), cx2) - cx1 # Constrain bounds and reset coordinate system\n",
    "    y1 = min(max(y1, cy1), cy2) - cy1\n",
    "    x2 = min(max(x2, cx1), cx2) - cx1\n",
    "    y2 = min(max(y2, cy1), cy2) - cy1\n",
    "    xmax, ymax = 224, 224\n",
    "\n",
    "    return np.array([synsetid, xmax, ymax, x1, y1, x2, y2]).astype(int)\n",
    "\n",
    "\n",
    "train_bbox_data = AttnDataset(TRAIN_DIR, BBOX_DIR, \n",
    "                              transform=IMG_TRANSFORMS, target_transform=BBOX_TRANSFORMS)\n",
    "train_bbox_loader = DataLoader(train_bbox_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "\n",
    "val_bbox_data = AttnDataset(VAL_DIR, BBOX_DIR, \n",
    "                            transform=IMG_TRANSFORMS, target_transform=BBOX_TRANSFORMS)\n",
    "val_bbox_loader = DataLoader(val_bbox_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "\n",
    "test_bbox_data = AttnDataset(TEST_DIR, BBOX_DIR, \n",
    "                             transform=IMG_TRANSFORMS, target_transform=BBOX_TRANSFORMS)\n",
    "test_bbox_loader = DataLoader(test_bbox_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "\n",
    "holdout_train_bbox_data = AttnDataset(HOLDOUT_TRAIN_DIR, BBOX_DIR, \n",
    "                                      transform=IMG_TRANSFORMS, target_transform=BBOX_TRANSFORMS)\n",
    "holdout_train_bbox_loader = DataLoader(holdout_train_bbox_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "\n",
    "holdout_val_bbox_data = AttnDataset(HOLDOUT_VAL_DIR, BBOX_DIR, \n",
    "                                    transform=IMG_TRANSFORMS, target_transform=BBOX_TRANSFORMS)\n",
    "holdout_val_bbox_loader = DataLoader(holdout_val_bbox_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "\n",
    "holdout_test_bbox_data = AttnDataset(HOLDOUT_TEST_DIR, BBOX_DIR, \n",
    "                                     transform=IMG_TRANSFORMS, target_transform=BBOX_TRANSFORMS)\n",
    "holdout_test_bbox_loader = DataLoader(holdout_test_bbox_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "print(\"Datasets loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Training loop for aux model\n",
    "\n",
    "from datetime import datetime\n",
    "from time import strftime\n",
    "\n",
    "def compute_iou(label_bboxes, output_bboxes):\n",
    "    \"\"\"\n",
    "    Given two bboxes a and b, computes the intersection over union loss\n",
    "    The bbox format I'm using is a tensor of length 6 in the format [width, height, xmin, ymin, xmax, ymax]\n",
    "    With batch size N this is an Nx6 tensor\n",
    "    \"\"\"\n",
    "    \n",
    "    a = label_bboxes # 6 values\n",
    "    b = output_bboxes.data # 4 values\n",
    "    \n",
    "    # https://stackoverflow.com/questions/27152904/calculate-overlapped-area-between-two-rectangles\n",
    "    # a = a.type(torch.cuda.FloatTensor)\n",
    "    # b = b.type(torch.cuda.FloatTensor)\n",
    "    \n",
    "    a = a[:,2:]\n",
    "    a_xmin, a_ymin, a_xmax, a_ymax = a[:,0], a[:,1], a[:,2], a[:,3]\n",
    "    b_xmin, b_ymin, b_xmax, b_ymax = b[:,0], b[:,1], b[:,2], b[:,3]\n",
    "    \n",
    "    # Intersection\n",
    "    xmin = torch.stack((a_xmin, b_xmin), dim=0)\n",
    "    xmax = torch.stack((a_xmax, b_xmax), dim=0)\n",
    "    ymin = torch.stack((a_ymin, b_ymin), dim=0)\n",
    "    ymax = torch.stack((a_ymax, b_ymax), dim=0)\n",
    "    \n",
    "    dx = (torch.min(xmax, 0)[0] - torch.max(xmin, 0)[0]).squeeze()\n",
    "    dy = (torch.min(ymax, 0)[0] - torch.max(ymin, 0)[0]).squeeze()\n",
    "    \n",
    "    # If (dx <= 0) or (dy <= 0), no intersection\n",
    "    mask1 = (dx >= 0)\n",
    "    mask2 = (dy >= 0)\n",
    "    mask = (mask1 * mask2).type(torch.cuda.FloatTensor)\n",
    "    intersection = dx * dy\n",
    "    intersection = intersection * mask\n",
    "    \n",
    "    # Union\n",
    "    area_a = (a_xmax - a_xmin) * (a_ymax - a_ymin)\n",
    "    area_b =  (b_xmax - b_xmin) * (b_ymax - b_ymin)\n",
    "    \n",
    "    union = area_a + area_b - intersection\n",
    "    iou = intersection / union\n",
    "    # print(torch.stack((area_a, area_b, intersection, union, iou),dim=0).transpose(1,0))\n",
    "    # print(torch.stack((a_xmin, a_ymin, a_xmax, a_ymax, b_xmin, b_ymin, b_xmax, b_ymax),dim=0).transpose(1,0))\n",
    "    # print(torch.stack((area_a, area_b, intersection, union, iou),dim=0).transpose(1,0))\n",
    "    return iou\n",
    "    \n",
    "\n",
    "def train_full_model(model, train_loader, val_loader, \n",
    "               checkpoints_path, log_path, lr=1e-2, start_epoch=0, session_id=0):\n",
    "        \n",
    "    NUM_EPOCHS = 50\n",
    "    # optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n",
    "    epoch = start_epoch\n",
    "    \n",
    "    while epoch < NUM_EPOCHS:\n",
    "        \n",
    "        print(\"Epoch \" + str(epoch))\n",
    "        \n",
    "        # Train\n",
    "        \"\"\"\n",
    "        print(\"  Train\")\n",
    "        model.train()\n",
    "        for minibatch, (x, y) in enumerate(train_loader):\n",
    "            \n",
    "            x_var = Variable(x.type(torch.cuda.FloatTensor))\n",
    "            label_bboxes = Variable(y[:,1:].type(torch.cuda.FloatTensor))\n",
    "            \n",
    "            (scores, bboxes) = model.forward(x_var) # Model's bbox outout only has 4 values per image\n",
    "            # This loss makes the aux net indifferent to misclassifications by the base model\n",
    "            loss_fn = torch.nn.MSELoss()\n",
    "            loss = loss_fn(bboxes, label_bboxes[:,2:])\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if minibatch % 10 == 0:\n",
    "                print(\"      \" + str(minibatch) + \" \" + str(float(loss.data.cpu().numpy())))\n",
    "        \"\"\"\n",
    "            \n",
    "        print(\"  Eval\")\n",
    "        iou_sum = torch.zeros(1).type(torch.cuda.FloatTensor)\n",
    "        imagenet_loc_sum = torch.zeros(1).type(torch.cuda.FloatTensor)\n",
    "        num_samples = torch.zeros(1).type(torch.cuda.FloatTensor)\n",
    "        model.eval()\n",
    "        for minibatch, (x, y) in enumerate(val_loader):\n",
    "            \n",
    "            x_var = Variable(x.type(torch.cuda.FloatTensor), volatile=True)\n",
    "            label_bboxes = y[:,1:].type(torch.cuda.FloatTensor)\n",
    "\n",
    "            (scores, bboxes) = model.forward(x_var)\n",
    "            iou = compute_iou(label_bboxes, bboxes)\n",
    "            iou_sum += torch.sum(iou)\n",
    "            \n",
    "            mask = (iou >= 0.5).type(torch.cuda.FloatTensor)\n",
    "            imagenet_loc_sum += torch.sum(iou * mask)\n",
    "            \n",
    "            num_samples += bboxes.size(0)\n",
    "            \n",
    "            mean_iou = float(iou_sum.cpu().numpy()[0]) / num_samples.cpu().numpy()[0]\n",
    "            mean_imagenet_loc = float(imagenet_loc_sum.cpu().numpy()[0]) / num_samples.cpu().numpy()[0]\n",
    "            \n",
    "            if minibatch % 10 == 0:\n",
    "                print(\"      \" + str(minibatch) + \" \" + str(mean_iou) + \" \" + str(mean_imagenet_loc))\n",
    "        \n",
    "        sid = str(session_id)\n",
    "        timenow = str(datetime.now().time())\n",
    "        \n",
    "        with open(log_path + sid + \".log\", 'a') as f:\n",
    "            f.write(\"\".join([sid, \"\\t\",\n",
    "                        str(epoch), \"\\t\",\n",
    "                        str(mean_iou), \"\\t\",\n",
    "                        timenow, \"\\n\"]))\n",
    "        \n",
    "        torch.save(model.state_dict(), checkpoints_path + \"session_\" + sid + \"_epoch_\" + str(epoch) + \".checkpoint\")\n",
    "        \n",
    "        epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from models import ResnetHost, AuxNet, AuxResNet\n",
    "\n",
    "N_TRAIN_CLASSES = 160\n",
    "\n",
    "# Load training host (resnet) and freeze all parameters\n",
    "train_host_resnet = ResnetHost(n_classes=N_TRAIN_CLASSES)\n",
    "train_host_resnet.load_state_dict(torch.load(\"data/models/train_resnet_session_0_epoch_10.checkpoint\"))\n",
    "train_host_resnet.freeze_weights()\n",
    "\n",
    "# Initialize new auxnet\n",
    "aux_net = AuxNet(spatial_size=28, channels=3)\n",
    "aux_net.set_retrain([\"conv\", \"fc\"], True)\n",
    "\n",
    "# Define the full network\n",
    "train_net = AuxResNet(train_host_resnet, aux_net)\n",
    "train_net.cuda()\n",
    "train_net.set_retrain([], True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# load weights\n",
    "train_net.load_state_dict(torch.load(\"data/models/train_full_session_10_epoch_1.checkpoint\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_full_model(train_net, train_bbox_loader, val_bbox_loader, \n",
    "               \"data/models/train_full_\", \"data/logs/\",\n",
    "                lr=1e-4, start_epoch=0, session_id=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from models import ResnetHost, AuxNet, AuxResNet\n",
    "\n",
    "N_TEST_CLASSES = 38\n",
    "\n",
    "# Load test host (resnet) and freeze all parameters\n",
    "test_host_resnet = ResnetHost(n_classes=N_TEST_CLASSES)\n",
    "test_host_resnet.load_state_dict(torch.load(\"data/models/test_resnet_session_0_epoch_16.checkpoint\"))\n",
    "test_host_resnet.freeze_weights()\n",
    "\n",
    "# Load trained auxnet\n",
    "aux_net = train_net.auxnet\n",
    "aux_net.freeze_weights()\n",
    "\n",
    "# Or, initialize new AuxNet\n",
    "# aux_net = AuxNet(spatial_size=28, channels=3)\n",
    "# aux_net.set_retrain([\"conv\", \"fc\"], True)\n",
    "\n",
    "test_net = AuxResNet(test_host_resnet, aux_net)\n",
    "test_net.cuda()\n",
    "train_net.set_retrain([], True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_net.load_state_dict(torch.load(\"data/models/test_newaux_full_session_0_epoch_4.checkpoint\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_full_model(test_net, holdout_train_bbox_loader, holdout_val_bbox_loader,\n",
    "                \"data/models/test_newaux_full_\", \"data/logs/\",\n",
    "                 lr=1e-3, start_epoch=2, session_id=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
