{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    A summary of the most common evaluation metrics:\n",
    "\n",
    "<img src=\"images/metrics.png\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "TensorFlow metrics module **`tf.metrics`**\n",
    "\n",
    "    tf.metrics.accuracy\n",
    "    tf.metrics.auc\n",
    "    tf.metrics.precision\n",
    "    tf.metrics.recall\n",
    "    tf.metrics.recall_at_k\n",
    "    tf.metrics.true_positives\n",
    "    tf.metrics.false_negatives\n",
    "    tf.metrics.false_positives\n",
    "    tf.metrics.mean_per_class_accuracy\n",
    "    tf.metrics.precision_at_thresholds\n",
    "    tf.metrics.recall_at_thresholds\n",
    "    \n",
    "TF-Keras metrics module **`tf.contrib.keras.metrics`**\n",
    "\n",
    "    tf_keras_metrics.binary_accuracy\n",
    "    tf_keras_metrics.categorical_accuracy\n",
    "    tf_keras_metrics.top_k_categorical_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy and Precision\n",
    "\n",
    "**Precision** is a description of **random errors**, a measure of **statistical variability**. In other words, the closeness of two or more measurements to each other.\n",
    "\n",
    "**Accuracy** is a description of **systematic errors**, a measure of **statistical bias**. In other words, the closeness of a measured value to the true value.\n",
    "\n",
    "\n",
    "<i>**Precision and Accuracy** are related to the **Bias-variance trade-off** found in Machine Learning models.</i>\n",
    "\n",
    "\n",
    "**High variance** can be caused by the model **overfitting** (low precision)\n",
    "\n",
    "*Solution:*\n",
    "- add regularization to the model\n",
    "- collect more data\n",
    "- decrease model expressiveness (complexity)\n",
    "- bagging (Bootstrap Aggregating) or other resampling techniques (random forest) \n",
    "\n",
    "**High bias** can be caused by **under-fitting** the model. (low accuracy)\n",
    "\n",
    "*Solution:*\n",
    "- increase model expressiveness (complexity)\n",
    "- collect more data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model predictions\n",
    "predictions = DeepLearningModel(inputs)\n",
    "\n",
    "# TensorFlow core accuracy metric\n",
    "accuracy = tf.metrics.accuracy(labels, predictions)\n",
    "\n",
    "# TF-Keras accuracy metrics\n",
    "accuracy = tf_keras_metrics.binary_accuracy(labels, predictions)\n",
    "\n",
    "# multi-class accuracy\n",
    "accuracy = tf_keras_metrics.categorical_accuracy(labels, predictions)\n",
    "\n",
    "# Top-K accuracy\n",
    "accuracy = tf_keras_metrics.top_k_categorical_accuracy(labels, predictions)\n",
    "\n",
    "# mean per class accuracy\n",
    "accuracy = tf.metrics.mean_per_class_accuracy(labels, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recall and F-1 Score\n",
    "\n",
    "<img src=\"images/recall.png\" width=\"200\">\n",
    "\n",
    "$$Recall = \\frac{TruePositive}{PositiveSamples} = \\frac{TruePositive}{TruePositive + FalseNegative} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model predictions\n",
    "predictions = DeepLearningModel(inputs)\n",
    "\n",
    "# TensorFlow core recall metric\n",
    "recall = tf.metrics.recall(labels, predictions)\n",
    "\n",
    "# Recall at k\n",
    "recall = tf.metrics.recall_at_k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Evaluation Metrics\n",
    "\n",
    "**F1 score is the harmonic mean of precision and recall**\n",
    "\n",
    "$${\\displaystyle F_{1}=2\\cdot {\\frac {1}{{\\tfrac {1}{\\mathrm {recall} }}+{\\tfrac {1}{\\mathrm {precision} }}}}=2\\cdot {\\frac {\\mathrm {precision} \\cdot \\mathrm {recall} }{\\mathrm {precision} +\\mathrm {recall} }}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# F1 score metric\n",
    "def F1_score(labels, predictions):\n",
    "    precision = tf.metrics.precision(labels, predictions)\n",
    "    recall = tf.metrics.recall(labels, predictions)\n",
    "    return 2 * tf.multiply(precision, recall) / tf.add(precision, recall)\n",
    "\n",
    "# recall metric\n",
    "def recall(labels, predictions):\n",
    "    TP = tf.metrics.true_positives(labels, predictions)\n",
    "    FN = tf.metrics.false_negatives(labels, predictions)\n",
    "    return TP / tf.add(TP,FN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# ROC AUC\n",
    "\n",
    "- An **ROC curve** is the most commonly used way to visualize the performance of a **binary classifier**. The curve is created by plotting the **true positive rate (TPR**) against the **false positive rate (FPR)** at various threshold settings. \n",
    "\n",
    "- **AUC** is a good way summarize the **classifier's performance** in a single number. This number is between 0.5 and 1. The **Area Under the Curve (AUC)** is literally just the percentage of the box that is under the curve. This metric quantifies the performance of a classifier into one number for model comparaison.\n",
    "\n",
    "\n",
    "<img src=\"images/roc1.png\" width=\"300\">\n",
    "\n",
    "\n",
    "- think of AUC as representing the **probability that a classifier will rank a randomly chosen positive observation higher than a randomly chosen negative observation**, and thus it is a useful metric even for datasets with **highly unbalanced classes**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model predictions\n",
    "predictions = DeepLearningModel(inputs)\n",
    "\n",
    "# TensorFlow core AUC metric\n",
    "auc = tf.metrics.auc(labels, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow Streaming Metrics -defining Multiple Metrics\n",
    "\n",
    "**Dictionary Aggregation**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model predictions\n",
    "predictions = DeepLearningModel(inputs)\n",
    "\n",
    "\n",
    "# Aggregates the value and update ops into dictionary:\n",
    "names_to_values, names_to_updates = tf.contrib.slim.metrics.aggregate_metric_map({\n",
    "    'eval/Accuracy': tf.metrics.accuracy(labels, predictions),\n",
    "    'eval/Precision': tf.metrics.precision(labels, predictions),\n",
    "    'eval/Recall': tf.metrics.recall(labels, predictions)\n",
    "})\n",
    "\n",
    "\n",
    "# Evaluate the model using 1000 batches of data:\n",
    "num_batches = 1000\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "\n",
    "    # run metrics over multiple batches\n",
    "    for batch_id in range(num_batches):\n",
    "        sess.run(names_to_updates.values())\n",
    "\n",
    "    # Get each metric end value\n",
    "    metric_values = sess.run(name_to_values.values())\n",
    "    for metric, value in zip(names_to_values.keys(), metric_values):\n",
    "        print('Metric %s has value: %f' % (metric, value))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
