{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) (20 points) The figure below shows three possible feature test for the root\n",
    "node of a decision tree to predict spam e-mail messages (based on the\n",
    "example discussed in class). \n",
    "\n",
    "- (a) Calculate the expected information gain for\n",
    "each feature test (please show your formula). \n",
    "\n",
    "- (b) Which feature test will be\n",
    "selected by the Decision Tree Learning algorithm?\n",
    "\n",
    "\n",
    "![](1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- feature set 1:\n",
    "    - entropy before split: $-\\frac{4}{9} \\log \\frac{4}{9} - \\frac{5}{9} \\log \\frac{5}{9} = 0.991$\n",
    "    - entropy after split: $-\\frac{4}{4} \\log \\frac{4}{4} - 0 - \\frac{4}{5} \\log \\frac{4}{5} - \\frac{1}{5} \\log \\frac{1}{5}= 0.722$\n",
    "    - info gain (entropy reduction) = $0.991 - 0.722 = 0.269$\n",
    "    \n",
    "    \n",
    "- feature set 2:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) (20 points) Construct a regular expression as the value for the parameter\n",
    "token_pattern so that CountVectorizer can extract hashtags, twitter user\n",
    "names (e.g., @realDonalTrump), and words from tweets as tokens. Explain\n",
    "how you construct the regular expression. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "regular expression:\n",
    "\n",
    "- first clean the urls using: \n",
    "\n",
    "$$\\text{ (https?|ftp|file)://.+}$$\n",
    "\n",
    "![](2.png)\n",
    "\n",
    "\n",
    "- then extract the hashtags, usernames and words in tweets using:\n",
    "\n",
    "$$\\text{[@#][a-zA-Z0-9_]+|[a-zA-Z]+}$$\n",
    "\n",
    "![](3.png)\n",
    "\n",
    "\n",
    "Note:\n",
    "\n",
    "- I could not figure out a tidy way to extract hashtags, usernames and words in tweets all together while excluding the urls because if the urls do not get cleaned up first, it would break up as individual words that do not make sense at all, such as, 'https://t.co/' => 'https' as a word, 't' as a word, 'co' as word\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
