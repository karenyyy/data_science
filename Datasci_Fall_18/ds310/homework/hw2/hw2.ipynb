{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problemset 2: K Nearest Neighbor and Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.(25 pts.) Suppose 7 nearest neighbor search for a query sample returns {7, 6, 8, 4, 7, 11, 9}\n",
    "as the values of the function at the 7 nearest data samples in the training set.\n",
    "\n",
    "- (a) Assuming prediction using 7 nearest neighbor interpolation, what is the predicted\n",
    "value of the function for the query sample?\n",
    "\n",
    "- (b) Assuming prediction using 7 nearest neighbor regression, what is the predicted\n",
    "value of the function for the query sample?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Since the nearest neighbor algorithm selects the value of the nearest point and does not consider the values of neighboring points at all, thus $\\arg \\min distance(x', x_i)$ is achieved at $i$ when $f(x_i)=7$, thus the predicted value of the function for the sample $x'$ should be $7$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) the result depends on the loss function applied, \n",
    "\n",
    "- for instance:\n",
    "    - L1 loss\n",
    "    $$\\min \\sum \\mid f(x') - f(x_i) \\mid$$\n",
    "    \n",
    "    in this case\n",
    "    \n",
    "    \n",
    "   $$\\arg \\min \\left [\\mid f(x') - 7 \\mid + \\mid f(x') - 6 \\mid +\\mid f(x') \\\\\n",
    "    - 8 \\mid+\\mid f(x') - 4 \\mid+\\mid f(x') - 7 \\mid+\\mid f(x') - 11 \\mid+\\mid f(x') - 9 \\mid \\right ]$$\n",
    "    \n",
    "   is achieved when $f(x')$ is the median of the array, thus $f(x') = 7$\n",
    "    \n",
    "   - L2 loss\n",
    "    \n",
    "     $$\\min L \\: where \\: L = \\sum \\left ( f(x') - f(x_i) \\right )^2$$\n",
    "     \n",
    "    in this case\n",
    "   \n",
    "\n",
    "   $$ \\arg \\min  \\left ( f(x') - 7\\right )^2+ \\left ( f(x') - 6 \\right )^2+ \\left ( f(x') - 8 \\right )^2+\\left ( f(x') - 4 \\right )^2+ \\left ( f(x') - 7 \\right )^2+ \\left ( f(x') - 11 \\right )^2+ \\left ( f(x') -9 \\right )^2$$\n",
    "    \n",
    "   is achieved when \n",
    "  \n",
    "\n",
    "$$\\frac{\\partial L}{\\partial f(x')} = 2 \\left [\\left ( f(x') - 7 \\right ) + \\\\\n",
    "\\left ( f(x') - 6 \\right )+ \\left ( f(x') - 8 \\right )+ \\left ( f(x') - 4 \\right )+ \\left ( f(x') - 7\\right )+ \\left ( f(x') -11 \\right )+ \\left ( f(x') - 9 \\right )\\right ]$$\n",
    "    \n",
    "$$=14 f(x') -2 \\cdot 52 = 0$$\n",
    "    \n",
    "$$f(x') = 7.429$$\n",
    "    \n",
    "(__also the mean of the array__)\n",
    "    \n",
    "thus the predicted value of the function for the sample $x'$ should be $7.429$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Q2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "2.(25 pts.) Alex is building a K nearest neighbor classifier for predicting the language\n",
    "to which a word belongs. The words use the Latin alphabet (which is used by English,\n",
    "Spanish, German and several other European languages). For better or for worse,\n",
    "Alex chooses to represent the words using three features: length of the word, number\n",
    "of vowels in the word, and whether or not the word ends in the letter ’n” (0 for No,\n",
    "1 for Yes). Suppose she has training data for two languages: regeln: German; pido:\n",
    "spanish.\n",
    "\n",
    "- (a) What is the dimensionality of the feature space?\n",
    "\n",
    "- (b) How are the two training data samples encoded in terms of the 3 features listed?\n",
    "\n",
    "- (c) Using 1-nearest neighbor classifier using the Manhattan distance metric, what is\n",
    "the prediction for the word ”neben”?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- (a) The dimensionality of the feature space is 3 because there are three engineered features in feature set: \n",
    "    - length of the word\n",
    "    - number of vowels in the word\n",
    "    - whether or not the word ends in the letter 'n'\n",
    "    \n",
    "- (b) \n",
    "\n",
    "|   | length of the word  | number of vowels in the word  |   whether or not the word ends in the letter 'n'|  \n",
    "|---|---|---|---|\n",
    "| regeln  | 6  | 2  |  1 | \n",
    "| pido  | 4  | 2  |  0 | \n",
    "\n",
    "- (c)\n",
    "\n",
    "the feature encoding for 'neben' is:\n",
    "\n",
    "|   | length of the word  | number of vowels in the word  |   whether or not the word ends in the letter 'n'|  \n",
    "|---|---|---|---|\n",
    "| neben  | 5  | 2  |  1 | \n",
    "\n",
    "----\n",
    "\n",
    " Using the Manhattan distance metric:\n",
    "\n",
    "$$distance = \\left|x_{1}-x_{2}\\right|+\\left|y_{1}-y_{2}\\right|+\\left|z_{1}-z_{2}\\right|$$\n",
    "\n",
    "Therefore:\n",
    "\n",
    "- 'neben' to 'regeln':\n",
    "\n",
    "$$distance_1 = \\left|5-6\\right|+\\left|2-2\\right|+\\left|1-1\\right| = 1$$\n",
    "\n",
    "- 'neben' to 'pido':\n",
    "\n",
    "$$distance_2 = \\left|5-4\\right|+\\left|2-2\\right|+\\left|1-0\\right| = 2$$\n",
    "\n",
    "So:\n",
    "\n",
    "$$\\min (distance_1, distance_2) = distance_1 = 1$$\n",
    "\n",
    "Thus 'neben' is more likely to be German"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Q3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "3.(25 pts.) Assume that you have a trained K nearest neighbor classifier.\n",
    "\n",
    "- (a) What is the runtime complexity of this classifying p data samples given a K\n",
    "nearest neighbor classifier trained on m training samples, each represented using\n",
    "d features?.\n",
    "\n",
    "- (b) Can you do better, using a carefully designed data structure with a K nearest\n",
    "neighbor search algorithm that is designed to take advantage of the data structure?\n",
    "Explain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- (a) the runtime complexity should be $O(dm)$\n",
    "\n",
    "Explain by steps:\n",
    "\n",
    "- for each of the p data samples\n",
    "\n",
    "    - step 1: distance with each of the training data points need to be calculated for later comparison, each calculation takes $O(d)$, thus for all m training data points, the runtime complexity is $O(dm)$\n",
    "\n",
    "    - step 2: then for all $m$ distance factor calculated, sorting should be applied before filtering out the top K shortest distance. Say if QuickSort is used here, then the runtime complexity is $O(m \\log m)$\n",
    "    \n",
    "    - step 3: next step, top K shortest distance are extracted, so $m$ comparisons are involved, with runtime complexity being $O(1)$\n",
    "    \n",
    "    - step 4: majority vote among the K nearest neighbors are taken, with complexity of $O(1)$\n",
    "    \n",
    "__Summary: __\n",
    "\n",
    "for each of the p data samples, a classification task takes runtime complexity of $O(dm) + O(m \\log m)$;\n",
    "\n",
    "for total p samples, the runtime complexity is $O(dm) + O(m \\log m)$\n",
    "\n",
    "\n",
    "- (b) As we can see, the part where we probably could optimize the runtime complexity would be __step 1__ where in part (a), we traverse the to-be-predicted sample across the whole $m$ training data point to calculate each distance, costing $O(dm)$ complexity\n",
    "\n",
    "we could use a more carefully designed data structure, like the __KD-Tree__ for this part in the KNN search algorithm by organize the m training data points into different segmentations.\n",
    "\n",
    "Basically how KD-Tree works:\n",
    "\n",
    "- If there is just one point, form a leaf with that\n",
    "point.\n",
    "\n",
    "- Otherwise, divide the points in half by a line perpendicular to one of the axes.\n",
    "\n",
    "- Recursively construct k-d trees for the two sets of points.\n",
    "\n",
    "\n",
    "Runtime complexity of constructing a KD-Tree:\n",
    "\n",
    "- first sort the points in each dimension, for each cost $O(m \\log m)$\n",
    "- so for d features, the complexity is $O(d m \\log m)$\n",
    "\n",
    "Runtime complexity of searching for top K nearest neighbors:\n",
    "\n",
    "Since $depth \\: of \\: KDTree = \\log m$\n",
    "\n",
    "- for each of the p data samples, it takes $O( \\log m)$ to get the K nearest neighbors\n",
    "- also at the last step, majority vote among the K nearest neighbors are taken, with complexity of $O(1)$\n",
    "\n",
    "\n",
    "__Summary: __\n",
    "\n",
    "- If construct a KD-Tree to organize the training data first then do the knn search\n",
    "    - the runtime complexity of KD-Tree is $O(d m \\log m)$\n",
    "    - the runtime complexity for knn searching is $O(\\log m)$\n",
    "    \n",
    "- __Note:__ thus especially if $d \\ll m$, for massive amount low-dimension (probably not optimal for spare though, because the construction of kd-tree for all the dimension might be too expensive) dataset, KD-Tree could be a more optimal option for KNN searching. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Q4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "4.(25 pts.) Consider approximation of $f(X)$ where $X \\in \\mathbb{R}^d$ given a training set $\\{(X_1, y_1), ..., (X_n, y_n)\\}$ using linear regression. That is, the goal is to find a linear approximation $y(X) = W \\cdot X + w_0$ where $W$ and $w_0$ are learnable parameters. Suppose we further know that the function is sparse, i.e., only a few of the d features define the behavior of f. How would you modify the objective function to be minimized to ensure that the resulting\n",
    "linear is sparse? (Hint: Add a term to the mean squared error that when minimized\n",
    "attempts to drive the weights to 0)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Since the function is sparse, with a few among all $d$ features that actually non-zero, thus to make sure the resulting linear is sparse as well, we could modify the loss function by adding a regularization term, a smoothing technique to prevent overfitting by bounding the weights. And in this case, to drive the weights to 0, it would be better if the added regularization term is __L1 regularization__.\n",
    "\n",
    "$$L = \\frac{1}{n} \\sum^n_{i=1} (y_i - \\hat {y_i})^2 = \\frac{1}{n} \\sum^n_{i=1} (y_i - w_i \\cdot x_i -w_0)^2$$\n",
    "\n",
    "\n",
    "Objective function:\n",
    "\n",
    "$$\\arg \\min _w \\left \\{ L + \\lambda ||w||_1 \\right \\}$$\n",
    "\n",
    "Explain: L1 regularization is better for sparse fucntion, because it is more likely to create 0-weights:\n",
    "\n",
    "----\n",
    "\n",
    "- mathematical proof:\n",
    "\n",
    "for a model consisting of the weights $(w_1,w_2,...,w_m)$\n",
    "\n",
    "1. With L1 regularization, the loss function $L_1(w) = \\frac{1}{n} \\sum^n_i|w_i|$\n",
    "\n",
    "2. With L2 regularization, the loss function $L_2(w) = \\frac{1}{n} \\sum_iw^2_i$\n",
    "\n",
    "Suppose we use gradient descent to train the model, the weights would be updated in the opposite direction of gradient, by the amount of step size times gradient, $w = w - \\eta \\cdot \\frac{\\partial L}{\\partial w}$\n",
    "\n",
    "This means that a more steep gradient will make us take a larger step, while a more flat gradient will make us take a smaller step. \n",
    "\n",
    "$$\\frac{\\partial L_1}{\\partial w_i} = \\frac{1}{n} \\cdot \\frac{w_i}{|w_i|}, where \\: \\frac{w_i}{|w_i|} = \\pm 1$$\n",
    "\n",
    "$$w_i= w_i - \\eta \\cdot \\frac{\\partial L}{\\partial w_i}, where \\: \\frac{\\partial L}{\\partial w_i}  is \\: independent \\: to \\: w$$\n",
    "\n",
    "which indicates that L1 regularization will approach 0 by updating the weights with the same learning rate, independent of w, so eventually it will drive the weights to 0.\n",
    "\n",
    "$$\\frac{\\partial L_2}{\\partial w_i} = \\frac{2}{n} w_i$$\n",
    "\n",
    "$$w_i = w_i - \\eta \\cdot \\frac{\\partial L}{\\partial w_i}, where \\: \\frac{\\partial L}{\\partial wi}  is \\: dependent \\: to \\: w$$\n",
    "\n",
    "So in L2 regularization, gradient $\\frac{L_2}{w}$ is linearly decreasing towards 0 as the weight goes towards 0. Therefore, it will take smaller and smaller steps as a weight approaches 0, but might be more difficult than L1 or never reach 0.\n",
    "\n",
    "----\n",
    "\n",
    "- intuitive:\n",
    "\n",
    "![](l1l2.png)\n",
    "\n",
    "We can see from the graph that for gradient descent contours for regression models, as the objective is to minimize the error function, we then focus on the intersection of the contours and the L1, L2 regions. It's apparent that the diamond region represented by L1 norm is more likely to create an intersection that cross the axes. \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
