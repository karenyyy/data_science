{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Gradient\tdescent\t/\tascent\t\tis\t\n",
    "guaranteed\tto\tfind\tthe\t\n",
    "minimum\t/\tmaximum\twhen\t\n",
    "the\tfunction\thas\ta\tsingle\t\n",
    "minimum\t/\tmaximum\t\n",
    "\n",
    "![](../images/lec2/3.png)\n",
    "\n",
    "\n",
    "Deduce the GD algorithm:\n",
    "\n",
    "![](../images/lec2/4.png)\n",
    "\n",
    "\n",
    "![](../images/lec2/5.png)\n",
    "\n",
    "\n",
    "$$E = \\frac{1}{2} \\sum_p e_p^2$$\n",
    "\n",
    "\n",
    "![](../images/lec2/6.png)\n",
    "\n",
    "\n",
    "![](../images/lec2/7.png)\n",
    "\n",
    "\n",
    "![](../images/lec2/8.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../images/lec2/9.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### from cs229 notes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![](../images/lec2/10.png)\n",
    "\n",
    "Notice how there are now two stages instead of a single stage. The relative importance is controlled by $\\beta$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Momentum update\n",
    "v = mu * v - learning_rate * dx # integrate velocity\n",
    "x += v # integrate position"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Locally weighted regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> why use LWR (non-parametric algorithm)?\n",
    "\n",
    "_In contrast, to\n",
    "make predictions using locally weighted linear regression, we need to keep\n",
    "the entire training set around._\n",
    "\n",
    "\n",
    "_The term “non-parametric” (roughly) refers\n",
    "to the fact that the amount of stuff we need to keep in order to represent the\n",
    "hypothesis h grows linearly with the size of the training set._\n",
    "\n",
    "----\n",
    "\n",
    "- simple linear regression too easy to underfit\n",
    "- polynomial regression too easy to overfit\n",
    "\n",
    "\n",
    "Since the choice of\n",
    "features is important to ensuring good performance of a learning algorithm\n",
    "\n",
    "----\n",
    "\n",
    "With the locally weighted linear regression (LWR) algorithm which, assuming\n",
    "there is sufficient training data, makes the choice of features less critical\n",
    "\n",
    "----\n",
    "\n",
    "In the __original linear regression algorithm__, to make a prediction at a query\n",
    "point x (i.e., to evaluate h(x)), we would:\n",
    "\n",
    "- Fit $\\theta$ to minimize $\\Sigma_i (y^i - \\theta^T x^{(i)})^2$\n",
    "- Output $\\theta^Tx$\n",
    "\n",
    "\n",
    "In contrast, the locally weighted linear regression algorithm does the following:\n",
    "\n",
    "- Fit $\\theta$ to minimize $\\Sigma_i w^{(i)}(y^i - \\theta^T x^{(i)})^2$\n",
    "- Output $\\theta^Tx$\n",
    "\n",
    "\n",
    "A fairly standard choice for the weights is:\n",
    "\n",
    "$$w^{(i)} = \\exp \\left ( - \\frac{(x^{(i)} - x)^2}{2 \\tau ^2} \\right )$$\n",
    "\n",
    "-  The parameter\n",
    "$\\tau$ controls how quickly the weight of a training example falls off with distance\n",
    "of its $x^{(i)}$ from the query point $x$;\n",
    "\n",
    "\n",
    "![](../images/lec2/11.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### LDA (Linear discriminant analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "`Fischer’s linear discriminant`\n",
    "\n",
    "__The idea is to find the line that best separates the 2 classes__\n",
    "\n",
    "![](../images/lec2/1.png)\n",
    "\n",
    "We have 2 classes that \n",
    "\n",
    "$$E_{X \\mid Y} \\left [ X \\mid Y = i \\right] = \\mu_i$$\n",
    "\n",
    "$$E_{X \\mid Y} \\left [ (X-\\mu_i)(X-\\mu_i)^T \\mid Y = i \\right] = \\Sigma_i$$\n",
    "\n",
    "And our goal is to find the line \n",
    "\n",
    "$$z = w^Tx$$\n",
    "\n",
    "that best separates them\n",
    "\n",
    "one possibility would be to maximize:\n",
    "\n",
    "$$(E_{Z \\mid Y} \\left [ Z \\mid Y = 1 \\right] - E_{Z \\mid Y} \\left [ Z \\mid Y = 0 \\right])^2=$$\n",
    "\n",
    "$$E_{X \\mid Y} \\left [ w^T x \\mid Y = 1 \\right] - E_{X \\mid Y} \\left [ w^T x \\mid Y = 0 \\right]$$\n",
    "\n",
    "$$=(w^T \\left [ \\mu_1 - \\mu_0 \\right])^2$$\n",
    "\n",
    "however, the method above could be made arbitrarily large by simply scaling w, so we need some type of normalization\n",
    "\n",
    "__Fischer suggested:__\n",
    "\n",
    "$$\\max_w \\frac{(E_{Z \\mid Y} \\left [ Z \\mid Y = 1 \\right]- E_{Z \\mid Y} \\left [ Z \\mid Y = 0 \\right])^2}{Var\\left [ Z \\mid Y = 1 \\right] + Var\\left [ Z \\mid Y = 0 \\right]}$$\n",
    "\n",
    "\n",
    "So\n",
    "$$J(w) = \\frac{(E_{Z \\mid Y} \\left [ Z \\mid Y = 1 \\right]- E_{Z \\mid Y} \\left [ Z \\mid Y = 0 \\right])^2}{Var\\left [ Z \\mid Y = 1 \\right] + Var\\left [ Z \\mid Y = 0 \\right]}$$\n",
    "\n",
    "$$J(w) = \\frac{w^T ( \\mu_1 - \\mu_0 ) ( \\mu_1 - \\mu_0 )^T w}{w^T (\\Sigma_1 + \\Sigma_0) w} = \\frac{w^T S_B w}{w^T S_W w}$$\n",
    "\n",
    "where:\n",
    "\n",
    "- between class scatter:\n",
    "$$S_B = (\\mu_1 - \\mu_0)(\\mu_1 - \\mu_0)^T$$\n",
    "\n",
    "- within class scatter:\n",
    "$$S_W = (\\Sigma_1 + \\Sigma_0)$$\n",
    "\n",
    "\n",
    "maximizing the ratio\n",
    "\n",
    "\n",
    "$$J(w) = \\frac{w^T S_B w}{w^T S_W w}$$\n",
    "\n",
    "\n",
    "is equivalent to \n",
    "- maximizing the numerator \n",
    "- keeping the denominator constant\n",
    "\n",
    "$$\\max w^T S_B w $$ subject to \n",
    "$$w^T S_W w = K$$\n",
    "\n",
    "Can be accomplished using Larange multipliers\n",
    "\n",
    "define the Lagrangian\n",
    "\n",
    "$$L = w^T S_B w - \\lambda (w^T S_w w - K)$$\n",
    "\n",
    "\n",
    "and maximize wirh respect to both w and $\\lambda$\n",
    "\n",
    "\n",
    "$$L = w^T (S_B - \\lambda S_w) w + \\lambda K$$\n",
    "\n",
    "\n",
    "with respect to w to 0 we get\n",
    "\n",
    "$$\\triangledown_w L = 2(S_B - \\lambda S_w)w = 0$$\n",
    "\n",
    "\n",
    "so \n",
    "\n",
    "$$S_B w = \\lambda S_w w$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$$S_w ^{-1} S_B w = \\lambda w$$\n",
    "\n",
    "$$S_w ^{-1}  (\\mu_1 - \\mu_0)(\\mu_1 - \\mu_0)^T  w = \\lambda w$$\n",
    "\n",
    "$$w^* = S_w ^{-1}  (\\mu_1 - \\mu_0) = (\\Sigma_1 + \\Sigma_0) ^{-1}  (\\mu_1 - \\mu_0)$$\n",
    "\n",
    "\n",
    "for a classification problem with Gaussian classes of equal cov $\\Sigma_i = \\Sigma$, the BDR boundary is the plane of normal\n",
    "\n",
    "$$w = \\Sigma^{-1} (\\mu_i - \\mu_j)$$\n",
    "\n",
    "\n",
    "![](../images/lec2/2.png)\n",
    "\n",
    "----\n",
    "\n",
    "### Takeaways:\n",
    "\n",
    "- it is optimal if and only if the classes are Gaussian and have optimal if and only if the classes are Gaussian and have\n",
    "equal covariance\n",
    "- better than PCA, but not necessarily good enough\n",
    "- a classifier on the LDA feature, is equivalent to\n",
    "     - the BDR after the approximation of the data by two Gaussians with equal covariance\n",
    "     \n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### LDA code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from utils.util import cal_covariance_matrix, train_test_split, accuracy_score\n",
    "from sklearn.datasets import load_iris\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class LDA(object):\n",
    "    def __init__(self):\n",
    "        self.w = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X1 = X[y == 0]\n",
    "        X2 = X[y == 1]\n",
    "\n",
    "        cov1 = cal_covariance_matrix(X1)\n",
    "        cov2 = cal_covariance_matrix(X2)\n",
    "        cov_tot = cov1 + cov2\n",
    "\n",
    "        mean1 = X1.mean(0)\n",
    "        mean2 = X2.mean(0)\n",
    "        mean_diff = np.atleast_1d(mean1 - mean2)\n",
    "\n",
    "        # Determine the vector which when X is projected onto it best separates the\n",
    "        # data by class. w = (mean1 - mean2) / (cov1 + cov2)\n",
    "        self.w = np.linalg.pinv(cov_tot).dot(mean_diff)\n",
    "\n",
    "    def transform(self, X, y):\n",
    "        self.fit(X, y)\n",
    "        X_transform = X.dot(self.w)\n",
    "        return X_transform\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred = []\n",
    "        for sample in X:\n",
    "            h = sample.dot(self.w)\n",
    "            y = 1 * (h < 0)\n",
    "            y_pred.append(y)\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "def main():\n",
    "    data = load_iris()\n",
    "    X = data.data\n",
    "    y = data.target\n",
    "\n",
    "    X = X[y != 2]\n",
    "    y = y[y != 2]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "    lda = LDA()\n",
    "    lda.fit(X_train, y_train)\n",
    "    y_pred = lda.predict(X_test)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "\n",
    "    plt.plot(X_test, y_pred)\n",
    "    plt.title('LDA')\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![](lda_iris_test.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### In class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The matrix is of the following shape: (442, 10)\n",
      "The label vector is of the following shape:  (442,)\n",
      "The feature matrix:\n",
      " [[ 0.03807591  0.05068012  0.06169621 ... -0.00259226  0.01990842\n",
      "  -0.01764613]\n",
      " [-0.00188202 -0.04464164 -0.05147406 ... -0.03949338 -0.06832974\n",
      "  -0.09220405]\n",
      " [ 0.08529891  0.05068012  0.04445121 ... -0.00259226  0.00286377\n",
      "  -0.02593034]\n",
      " ...\n",
      " [ 0.04170844  0.05068012 -0.01590626 ... -0.01107952 -0.04687948\n",
      "   0.01549073]\n",
      " [-0.04547248 -0.04464164  0.03906215 ...  0.02655962  0.04452837\n",
      "  -0.02593034]\n",
      " [-0.04547248 -0.04464164 -0.0730303  ... -0.03949338 -0.00421986\n",
      "   0.00306441]]\n",
      "The labels:\n",
      " [151.  75. 141. 206. 135.  97. 138.  63. 110. 310. 101.  69. 179. 185.\n",
      " 118. 171. 166. 144.  97. 168.  68.  49.  68. 245. 184. 202. 137.  85.\n",
      " 131. 283. 129.  59. 341.  87.  65. 102. 265. 276. 252.  90. 100.  55.\n",
      "  61.  92. 259.  53. 190. 142.  75. 142. 155. 225.  59. 104. 182. 128.\n",
      "  52.  37. 170. 170.  61. 144.  52. 128.  71. 163. 150.  97. 160. 178.\n",
      "  48. 270. 202. 111.  85.  42. 170. 200. 252. 113. 143.  51.  52. 210.\n",
      "  65. 141.  55. 134.  42. 111.  98. 164.  48.  96.  90. 162. 150. 279.\n",
      "  92.  83. 128. 102. 302. 198.  95.  53. 134. 144. 232.  81. 104.  59.\n",
      " 246. 297. 258. 229. 275. 281. 179. 200. 200. 173. 180.  84. 121. 161.\n",
      "  99. 109. 115. 268. 274. 158. 107.  83. 103. 272.  85. 280. 336. 281.\n",
      " 118. 317. 235.  60. 174. 259. 178. 128.  96. 126. 288.  88. 292.  71.\n",
      " 197. 186.  25.  84.  96. 195.  53. 217. 172. 131. 214.  59.  70. 220.\n",
      " 268. 152.  47.  74. 295. 101. 151. 127. 237. 225.  81. 151. 107.  64.\n",
      " 138. 185. 265. 101. 137. 143. 141.  79. 292. 178.  91. 116.  86. 122.\n",
      "  72. 129. 142.  90. 158.  39. 196. 222. 277.  99. 196. 202. 155.  77.\n",
      " 191.  70.  73.  49.  65. 263. 248. 296. 214. 185.  78.  93. 252. 150.\n",
      "  77. 208.  77. 108. 160.  53. 220. 154. 259.  90. 246. 124.  67.  72.\n",
      " 257. 262. 275. 177.  71.  47. 187. 125.  78.  51. 258. 215. 303. 243.\n",
      "  91. 150. 310. 153. 346.  63.  89.  50.  39. 103. 308. 116. 145.  74.\n",
      "  45. 115. 264.  87. 202. 127. 182. 241.  66.  94. 283.  64. 102. 200.\n",
      " 265.  94. 230. 181. 156. 233.  60. 219.  80.  68. 332. 248.  84. 200.\n",
      "  55.  85.  89.  31. 129.  83. 275.  65. 198. 236. 253. 124.  44. 172.\n",
      " 114. 142. 109. 180. 144. 163. 147.  97. 220. 190. 109. 191. 122. 230.\n",
      " 242. 248. 249. 192. 131. 237.  78. 135. 244. 199. 270. 164.  72.  96.\n",
      " 306.  91. 214.  95. 216. 263. 178. 113. 200. 139. 139.  88. 148.  88.\n",
      " 243.  71.  77. 109. 272.  60.  54. 221.  90. 311. 281. 182. 321.  58.\n",
      " 262. 206. 233. 242. 123. 167.  63. 197.  71. 168. 140. 217. 121. 235.\n",
      " 245.  40.  52. 104. 132.  88.  69. 219.  72. 201. 110.  51. 277.  63.\n",
      " 118.  69. 273. 258.  43. 198. 242. 232. 175.  93. 168. 275. 293. 281.\n",
      "  72. 140. 189. 181. 209. 136. 261. 113. 131. 174. 257.  55.  84.  42.\n",
      " 146. 212. 233.  91. 111. 152. 120.  67. 310.  94. 183.  66. 173.  72.\n",
      "  49.  64.  48. 178. 104. 132. 220.  57.]\n",
      "Prediction of regression on the test sample: [198.62598197]\n",
      "Prediction using local regression: [137.66666667]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets, linear_model\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "'''Import the Diabetes dataset'''\n",
    "diabetes = datasets.load_diabetes()\n",
    "\n",
    "'''Info about the dataset'''\n",
    "print('The matrix is of the following shape:', diabetes.data.shape)\n",
    "print('The label vector is of the following shape: ', diabetes.target.shape)\n",
    "\n",
    "'''See what it looks like'''\n",
    "print('The feature matrix:\\n',diabetes.data)\n",
    "print('The labels:\\n',diabetes.target)\n",
    "\n",
    "'''The regressor'''\n",
    "my_regressor = linear_model.LinearRegression()\n",
    "\n",
    "'''Note: we have not taught systematic training yet, consider this as a simple example.\n",
    "    Later in the semester, we will learn how to properly train the machine learning models.\n",
    "'''\n",
    "train_data = diabetes.data\n",
    "train_labels = diabetes.target\n",
    "my_regressor.fit(train_data, train_labels) # train the classifier\n",
    "\n",
    "test_sample = [[0.011, 0.043, 0.056, 0.081, 0.075, 0.064, 0.053, 0.015, 0.023, 0.076]]\n",
    "\n",
    "'''Testing phase:'''\n",
    "print('Prediction of regression on the test sample:', my_regressor.predict(test_sample))\n",
    "\n",
    "'''Locally weighted regression'''\n",
    "my_knn_regressor = KNeighborsRegressor(n_neighbors = 3)# 3 neighbors chosen arbitrarily\n",
    "my_knn_regressor.fit(train_data, train_labels) # train the local regressor\n",
    "print('Prediction using local regression:', my_knn_regressor.predict(test_sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Reorganized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_squared_log_error\n",
    "\n",
    "\n",
    "def load_dataset(all_features=True, dim=2):\n",
    "    diabetes = datasets.load_diabetes()\n",
    "    y = diabetes.target\n",
    "    if all_features:\n",
    "        X = diabetes.data\n",
    "    else:\n",
    "        X = diabetes.data[:, np.newaxis, dim]\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def split_dataset(X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "def plot_scatter(X_train, y_train, X_test, y_test):\n",
    "    cmap = plt.get_cmap('viridis')\n",
    "    m1 = plt.scatter(x=X_train, y=y_train, color=cmap(0.9), s=10)\n",
    "    m2 = plt.scatter(x=X_test, y=y_test, color=cmap(0.5), s=10)\n",
    "    plt.legend((m1, m2), (\"Training data\", \"Test data\"), loc='lower right')\n",
    "    return plt\n",
    "\n",
    "\n",
    "class Regression(object):\n",
    "    def __init__(self, X_train, X_test, y_train, y_test):\n",
    "        self.X_train = X_train\n",
    "        self.X_test = X_test\n",
    "        self.y_train = y_train\n",
    "        self.y_test = y_test\n",
    "        self.regression_model = None\n",
    "\n",
    "    def fit(self):\n",
    "        self.regression_model.fit(self.X_train, self.y_train)\n",
    "\n",
    "    def predict(self):\n",
    "        return self.regression_model.predict(self.X_test)\n",
    "\n",
    "    def loss(self, y_real, y_pred, metric):\n",
    "        if metric == 'mse':\n",
    "            error = mean_squared_error(y_true=y_real, y_pred=y_pred)\n",
    "        elif metric == 'mse_log':\n",
    "            error = mean_squared_log_error(y_true=y_real, y_pred=y_pred)\n",
    "        elif metric == 'mse_abs':\n",
    "            error = mean_absolute_error(y_true=y_real, y_pred=y_pred)\n",
    "        return error\n",
    "\n",
    "    def plot_line(self, X, y_pred, plt, error, metric):\n",
    "        return NotImplemented\n",
    "\n",
    "\n",
    "class LinearRegression(Regression):\n",
    "    def __init__(self, X_train, X_test, y_train, y_test):\n",
    "        super(LinearRegression, self).__init__(X_train, X_test, y_train, y_test)\n",
    "        self.regression_model = linear_model.LinearRegression()\n",
    "\n",
    "    def fit(self):\n",
    "        super(LinearRegression, self).fit()\n",
    "\n",
    "    def predict(self):\n",
    "        return super(LinearRegression, self).predict()\n",
    "\n",
    "    def plot_line(self, X, y_pred, plt, error, metric):\n",
    "        plt.plot(X,\n",
    "                 y_pred,\n",
    "                 color='black',\n",
    "                 linewidth=2,\n",
    "                 label=\"Prediction\")\n",
    "        plt.title(\"linear-\" + metric + \": {0:.7g}\\n\".format(error), fontsize=10)\n",
    "\n",
    "\n",
    "class KNNRegression(Regression):\n",
    "    def __init__(self, X_train, X_test, y_train, y_test,\n",
    "                 dist='euclidean',\n",
    "                 neighbors=20,\n",
    "                 weights='uniform',\n",
    "                 algorithm='auto'):\n",
    "        super(KNNRegression, self).__init__(X_train, X_test, y_train, y_test)\n",
    "        self.dist = dist\n",
    "        self.neighbors = neighbors\n",
    "        self.weights = weights\n",
    "        self.algorithm = algorithm\n",
    "        self.regression_model = KNeighborsRegressor(n_neighbors=self.neighbors,\n",
    "                                                    weights=self.weights,\n",
    "                                                    algorithm=self.algorithm,\n",
    "                                                    metric=self.dist)\n",
    "\n",
    "    def fit(self):\n",
    "        super(KNNRegression, self).fit()\n",
    "\n",
    "    def predict(self):\n",
    "        return super(KNNRegression, self).predict()\n",
    "\n",
    "    def plot_line(self, X, y_pred, plt, error, metric):\n",
    "        plt.plot(X,\n",
    "                 y_pred,\n",
    "                 color='red',\n",
    "                 linewidth=0.1,\n",
    "                 label=\"Prediction\")\n",
    "        plt.title(\"knn-\" + metric + \": {0:.7g}\\n\".format(error), fontsize=10)\n",
    "\n",
    "\n",
    "def grid_search_comparison(lr_err, y_test, metric='mse', neighbors=200):\n",
    "    kr_errs = []\n",
    "    for neighbor in range(3, neighbors):\n",
    "        kr = KNNRegression(neighbors=neighbor)\n",
    "        kr.fit()\n",
    "        y_pred = kr.predict()\n",
    "        kr_err = kr.loss(y_real=y_test, y_pred=y_pred, metric=metric)\n",
    "        kr_errs.append(kr_err)\n",
    "\n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    plt.plot(range(3, neighbors), kr_errs)\n",
    "    plt.hlines(lr_err, xmin=1, xmax=neighbors, colors='red')\n",
    "    plt.title('Loss: {} Neighbors: {}'.format(metric, neighbors))\n",
    "    return fig\n",
    "\n",
    "\n",
    "def test_all_features():\n",
    "    metric = ['mse', 'mse_log', 'mse_abs']\n",
    "    X, y = load_dataset()\n",
    "    X_train, X_test, y_train, y_test = split_dataset(X, y)\n",
    "\n",
    "    lr = LinearRegression()\n",
    "    lr.fit()\n",
    "    lr_y_pred = lr.predict()\n",
    "\n",
    "    lr_mse = lr.loss(y_real=y_test, y_pred=lr_y_pred, metric=metric[0])\n",
    "    lr_mse_log = lr.loss(y_real=y_test, y_pred=lr_y_pred, metric=metric[1])\n",
    "    lr_mse_abs = lr.loss(y_real=y_test, y_pred=lr_y_pred, metric=metric[2])\n",
    "    lr_err = [lr_mse, lr_mse_log, lr_mse_abs]\n",
    "\n",
    "    for idx, m in enumerate(metric):\n",
    "        fig = grid_search_comparison(lr_err=lr_err[idx], y_test=y_test, metric=m)\n",
    "        fig.savefig(fname='plots/{}.png'.format(m))\n",
    "\n",
    "\n",
    "def test_single_feature():\n",
    "    n_features = 10\n",
    "    metric = ['mse', 'mse_log', 'mse_abs']\n",
    "\n",
    "    for d in range(2, n_features):\n",
    "        fig = plt.figure(figsize=(8, 8))\n",
    "\n",
    "        X, y = load_dataset(all_features=False, dim=d)\n",
    "        X_train, X_test, y_train, y_test = split_dataset(X, y)\n",
    "        plt_ = plot_scatter(X_train=X_train,\n",
    "                            X_test=X_test,\n",
    "                            y_train=y_train,\n",
    "                            y_test=y_test)\n",
    "\n",
    "        lr = LinearRegression(X_train, X_test, y_train, y_test)\n",
    "        lr.fit()\n",
    "        lr_y_pred = lr.predict()\n",
    "\n",
    "        kr = KNNRegression(X_train, X_test, y_train, y_test, neighbors=20)\n",
    "        kr.fit()\n",
    "        kr_y_pred = kr.predict()\n",
    "\n",
    "        for me in metric:\n",
    "            lr_err = lr.loss(y_real=y_test, y_pred=lr_y_pred, metric=me)\n",
    "            kr_err = kr.loss(y_real=y_test, y_pred=kr_y_pred, metric=me)\n",
    "            lr.plot_line(X=X_test, y_pred=lr_y_pred, plt=plt_, metric=me, error=lr_err)\n",
    "            kr.plot_line(X=X_test, y_pred=kr_y_pred, plt=plt_, metric=me, error=kr_err)\n",
    "            fig.savefig(fname='out_pred_plots/' + me + '-feature{}.png'.format(d))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    test_single_feature()\n",
    "    test_all_features()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
