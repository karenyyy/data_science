{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Q1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "__Logistic Regression:__\n",
    "\n",
    "1. Consider a logistic regression model parameterized by $\\theta_0= 14$ and $\\theta_1= -0.14$ and\n",
    "the following sample data. \n",
    "\n",
    "![](1.png)\n",
    "\n",
    "- a) (10%) Calculate the probability that $y = 1$ for each $x_i$ of the data set $(ℎ_{\\theta }(x))$.\n",
    "- b) (10%) Calculate the value of objective (cost) function based on idea of SSE,\n",
    "- c) (10%) Calculate the value of objective (cost) function based on log of Sigmoid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "For logistic regression learning:\n",
    "\n",
    "(a) \n",
    "\n",
    "$$h_\\theta(x_1) = P(y=1|x_1, \\theta) = \\frac{1}{1+ e^{-\\theta^T x}} = \\frac{1}{e^{-\\theta_0 - \\theta_1 \\cdot x_1}}$$\n",
    "\n",
    "$$ = \\frac{1}{e^{-14 + 0.14 \\cdot 80}} = 0.943$$\n",
    "\n",
    "\n",
    "$$h_\\theta(x_2) = P(y=1|x_2, \\theta) = \\frac{1}{1+ e^{-\\theta^T x}} = \\frac{1}{e^{-\\theta_0 - \\theta_1 \\cdot x_2}}$$\n",
    "\n",
    "$$ = \\frac{1}{e^{-14 + 0.14 \\cdot 20}} = 1.000$$\n",
    "\n",
    "\n",
    "$$h_\\theta(x_3) = P(y=1|x, \\theta) = \\frac{1}{1+ e^{-\\theta^T x}} = \\frac{1}{e^{-\\theta_0 - \\theta_1 \\cdot x_3}}$$\n",
    "\n",
    "$$ = \\frac{1}{e^{-14 + 0.14 \\cdot 120}} = 0.057$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "(b)\n",
    "\n",
    "Object Function based on the idea of SSE:\n",
    "\n",
    "$$J(\\theta) = J(\\theta_0, \\theta_1, ..., \\theta_n) = \\frac{1}{2m} \\sum^m_{i=1} (h_{\\theta}(x^i) - y^i)^2, \\text{ where } m = 3$$\n",
    "\n",
    "\n",
    "Thus\n",
    "\n",
    "$$J(\\theta) = \\frac{1}{2m} \\sum^m_{i=1} (h_{\\theta}(x^i) - y^i)^2 = \\frac{1}{6} [ (0.943 - 1)^2 + (1.00 -1)^2 + (0.057 - 0)^2 ] = 0.001083$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "(c)\n",
    "\n",
    "Object Function based on the idea of log of sigmoid:\n",
    "\n",
    "\\begin{align}\n",
    "Err(h_{\\theta}(x), y) = \n",
    "\\left\\{\\begin{matrix} - \\log(h_{\\theta}(x)), if \\: y=1\n",
    "\\\\ -\\log(1-h_{\\theta}(x)), if \\: y=0\n",
    "\\end{matrix}\\right.\n",
    "\\end{align}\n",
    "\n",
    "SO\n",
    "\n",
    "\\begin{equation}\\label{eq:}\n",
    "\\begin{aligned}\n",
    "J(h_{\\theta}(x), y) &= \\frac{1}{m} \\sum^m_{i=1} Err(h_{\\theta}(x), y) \\\\\n",
    "&= - \\frac{1}{m} \\sum^m_{i=1}\\left [ y ^i \\log(h_{\\theta}(x^i)) + (1-y^i) (\\log(1-h_{\\theta}(x^i)) \\right ] \\\\\n",
    "&= - \\frac{1}{m} [1 \\cdot \\log(0.943) + 1 \\cdot \\log 1 + 1 \\cdot \\log(1-0.057)] \\\\\n",
    "&= 0.0564\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Decision Tree__\n",
    "\n",
    "2. Consider the following data set for binary class problem.\n",
    "\n",
    "![](2.png)\n",
    "\n",
    "\n",
    "- a. (15%) Calculate the information gain (based on entropy) when splitting on A\n",
    "and B. Which attribute would the decision tree induction algorithm choose?\n",
    "\n",
    "- b. (15%) Calculate the gain (based on the Gini index) when splitting on A and B.\n",
    "Which attribute would the decision tree induction algorithm choose?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) \n",
    "\n",
    " \\begin{tabular}{|r|l|l|}\n",
    "  & A=T & A=F \\\\\n",
    "\\hline\n",
    "\t C1=+ &  4     & 0 \\\\\n",
    "\t C2=-   &  3    & 3 \\\\\n",
    "\\end{tabular}\n",
    "\n",
    "\n",
    "\n",
    " \\begin{tabular}{|r|l|l|}\n",
    "  & B=T & B=F \\\\\n",
    "\\hline\n",
    "\t C1=+ &  3    & 1 \\\\\n",
    "\t C2=-   &  2    & 4 \\\\\n",
    "\\end{tabular}\n",
    "\n",
    "\n",
    "\n",
    "- entropy before split:\n",
    "    - $$E = -0.4 \\log (0.4) - 0.6 \\log (0.6) = 0.971$$\n",
    "    - entropy after splitting on A:\n",
    "        - $$E(A_T) = -\\frac{4}{7} \\log \\frac{4}{7} - \\frac{3}{7} \\log \\frac{3}{7} = 0.985$$\n",
    "        - $$E(A_F) = - \\frac{3}{3} \\log \\frac{3}{3} = 0 $$\n",
    "        - info gain(A): $$E - \\frac{7}{10} E(A_T) - \\frac{3}{10} E(A_F) = 0.971 - 0.7 \\cdot 0.985 = 0.2815$$\n",
    "    - entropy after splitting on B:\n",
    "        - $$E(B_T) = -\\frac{3}{5} \\log \\frac{3}{5} - \\frac{2}{5} \\log \\frac{2}{5} = 0.971$$\n",
    "        - $$E(B_F) = - \\frac{1}{5} \\log \\frac{4}{5} = 0.722$$\n",
    "        - info gain(B): $$E - \\frac{5}{10} E(B_T) - \\frac{5}{10} E(B_F) = 0.971 - 0.5 \\cdot 0.971 - 0.5 \\cdot 0.722 = 0.1245$$\n",
    "        \n",
    "        \n",
    "\n",
    "Since $info \\: gain(A) = 0.2815 > info \\: gain(B) = 0.1245$, so if split on A, the reduction of entropy, i.e. the reduction of uncertainty is larger than if split on B. Thus __A should be selected for the Decision Tree.__\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b)\n",
    "\n",
    "-  Gini before split:\n",
    "    - $$G = 1- 0.4^2 - 0.6^2 = 0.48$$\n",
    "    - Gini after splitting on A:\n",
    "        - $$G(A_T) = 1-(\\frac{4}{7})^2 - (\\frac{3}{7}) ^2 = 0.4898$$\n",
    "        - $$G(A_F) = 1 - (\\frac{3}{3})^2 - (\\frac{0}{3})^2 = 0 $$\n",
    "        - info gain(A): $$G - \\frac{7}{10} G(A_T) - \\frac{3}{10} G(A_F) = 0.48 - 0.7 \\cdot 0.4898= 0.137$$\n",
    "    - Gini after splitting on B:\n",
    "        - $$G(B_T) = 1 -(\\frac{3}{5})^2 - (\\frac{2}{5})^2 = 0.48$$\n",
    "        - $$G(B_F) = 1- (\\frac{1}{5})^2 - (\\frac{4}{5})^2 = 0.32$$\n",
    "        - info gain(B): $$G - \\frac{5}{10} G(B_T) - \\frac{5}{10} G(B_F) = 0.48 - 0.5 \\cdot 0.48 - 0.5 \\cdot 0.32 = 0.08$$\n",
    "        \n",
    "        \n",
    "\n",
    "Since $info \\: gain(A) = 0.137 > info \\: gain(B) = 0.08$, so if split on A, the reduction of Gini, i.e. the reduction of uncertainty is larger than if split on B. Thus __A should be selected for the Decision Tree.__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following table summarizes a data set with three attributes A, B, C and two class\n",
    "labels +, − . Build a two-level decision tree.\n",
    "\n",
    "![](3.png)\n",
    "\n",
    "- a. (15%) According to the classification error rate, which attribute would be\n",
    "chosen as the first splitting attribute? For each attribute, show the\n",
    "contingency table (i.e., count matrix) and the gains in classification error\n",
    "rate.\n",
    "- b. (15%) Repeat for the two children of the root node.\n",
    "- c. (10%) How many instances are misclassified by the resulting decision\n",
    "tree?\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) \n",
    "\n",
    "\n",
    " \\begin{tabular}{|r|l|l|}\n",
    "  & A=T & A=F \\\\\n",
    "\\hline\n",
    "\t C1=+ &  25     & 25 \\\\\n",
    "\t C2=-   &  0    & 50 \\\\\n",
    "\\end{tabular}\n",
    "\n",
    "\n",
    "\n",
    " \\begin{tabular}{|r|l|l|}\n",
    "  & B=T & B=F \\\\\n",
    "\\hline\n",
    "\t C1=+ &  30    & 20 \\\\\n",
    "\t C2=-   &  20    & 30 \\\\\n",
    "\\end{tabular}\n",
    "\n",
    "\n",
    " \\begin{tabular}{|r|l|l|}\n",
    "  & C=T & C=F \\\\\n",
    "\\hline\n",
    "\t C1=+ &  25    & 25 \\\\\n",
    "\t C2=-   &  30    & 20 \\\\\n",
    "\\end{tabular}\n",
    "\n",
    "\n",
    "\n",
    "- Error before split:\n",
    "    - $$Err = 1-\\max(\\frac{50}{100}, \\frac{50}{100}) = 0.5$$\n",
    "    - Error after splitting on A:\n",
    "        - $$Err(A_T) = 1-\\max(\\frac{25}{25}, \\frac{0}{25}) = 0$$\n",
    "        - $$Err(A_F) = 1-\\max(\\frac{25}{75}, \\frac{50}{75}) = 0.333$$\n",
    "        - $$\\Delta Err(A) = Err- \\frac{25}{100} \\cdot Err(A_T) - \\frac{75}{100} \\cdot Err(A_F) = 0.5 - 0.75 \\cdot 0.333 = 0.25025$$\n",
    "    - Error after splitting on B:\n",
    "        - $$Err(B_T) = 1-\\max(\\frac{30}{50}, \\frac{20}{50}) = 0.4$$\n",
    "        - $$Err(B_F) = 1-\\max(\\frac{20}{50}, \\frac{30}{50}) = 0.4$$\n",
    "        - $$\\Delta Err(B) = Err- \\frac{50}{100} \\cdot Err(B_T) - \\frac{50}{100} \\cdot Err(B_F) = 0.5 - 0.5 \\cdot 0.4 \\cdot 2 = 0.1$$\n",
    "    - Error after splitting on C:\n",
    "        - $$Err(C_T) = 1-\\max(\\frac{25}{55}, \\frac{30}{55}) = 0.455$$\n",
    "        - $$Err(C_F) = 1-\\max(\\frac{25}{45}, \\frac{20}{45}) = 0.444$$\n",
    "        - $$\\Delta Err(C) = Err- \\frac{55}{100} \\cdot Err(C_T) - \\frac{45}{100} \\cdot Err(C_F) = 0.5 - 0.55 \\cdot 0.455 - 0.45 \\cdot 0.444 = 0.04995$$\n",
    "        \n",
    "Since $\\Delta Err(A) > \\Delta Err(B) > \\Delta Err(C) $, thus attribute A should be chosen as the first splitting attribute.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b)  \n",
    "\n",
    "- first child\n",
    "    - $A=T$ is pure and no further split is required\n",
    "    - $A=F$\n",
    "        \n",
    "        \n",
    "\\begin{tabular}{|r|l|l|}\n",
    "              & B=T & B=F \\\\\n",
    "            \\hline\n",
    "                 C1=+ &  25    & 0 \\\\\n",
    "                 C2=-   &  20    & 30 \\\\\n",
    "\\end{tabular}\n",
    "\n",
    "\n",
    "\\begin{tabular}{|r|l|l|}\n",
    "              & C=T & C=F \\\\\n",
    "            \\hline\n",
    "                 C1=+ &  0    & 25 \\\\\n",
    "                 C2=-   &  30    & 20 \\\\\n",
    "\\end{tabular}\n",
    "\n",
    "\n",
    "\n",
    "- Error before split:\n",
    "    - $$Err = 1-\\max(\\frac{45}{75}, \\frac{30}{75}) = 0.4$$\n",
    "- Error after splitting on B:\n",
    "    - $$Err(B_T) = 1-\\max(\\frac{25}{45}, \\frac{20}{45}) = 0.444$$\n",
    "    - $$Err(B_F) = 1-\\max(\\frac{0}{30}, \\frac{30}{30}) = 0$$\n",
    "    - $$\\Delta Err(B) = Err- \\frac{45}{75} \\cdot Err(B_T) - \\frac{30}{75} \\cdot Err(B_F) = 0.4 - 0.6  \\cdot 0.444 - 0.4 \\cdot 0 = 0.1336$$\n",
    "    \n",
    "    \n",
    "- Error after splitting on C:\n",
    "    - $$Err(C_T) = 1-\\max(\\frac{0}{30}, \\frac{30}{30}) = 0$$\n",
    "    - $$Err(C_F) = 1-\\max(\\frac{25}{45}, \\frac{20}{45}) = 0.444$$\n",
    "    - $$\\Delta Err(C) = Err- \\frac{30}{75} \\cdot Err(C_T) - \\frac{45}{75} \\cdot Err(C_F) = 0.4 - 0.4 \\cdot 0 - 0.6 \\cdot 0.444 = 0.1336$$\n",
    "        \n",
    "        \n",
    "Since $\\Delta Err(B) > \\Delta Err(C)$, thus either attribute B or C could be chosen as the next splitting attribute since their gain of error rate is the same\n",
    "\n",
    "\n",
    "\n",
    "If B is selected as the second child of the tree, then since $B=F$ is pure, so $B=T$ should be splitted\n",
    "\n",
    "\n",
    "\n",
    "\\begin{tabular}{|r|l|l|}\n",
    "              & C=T & C=F \\\\\n",
    "            \\hline\n",
    "                 C1=+ &  0    & 25 \\\\\n",
    "                 C2=-   &  20    & 0 \\\\\n",
    "\\end{tabular}\n",
    "\n",
    "\n",
    "both branches are pure.\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) \n",
    "\n",
    "from the last part\n",
    "\n",
    "- if B is the second child\n",
    "![](4.png)\n",
    "\n",
    "__misclassified instances = 0__\n",
    "\n",
    "- if C is the second child\n",
    "\n",
    "![](5.png)\n",
    "\n",
    "\n",
    "__misclassified instances = 0__\n",
    "\n",
    "\n",
    "So, in conclusion, there's no misclassified instances from this decision tree.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
